{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:01:15.413092Z",
     "iopub.status.busy": "2020-11-29T08:01:15.412161Z",
     "iopub.status.idle": "2020-11-29T08:01:59.302334Z",
     "shell.execute_reply": "2020-11-29T08:01:59.301565Z"
    },
    "papermill": {
     "duration": 43.937243,
     "end_time": "2020-11-29T08:01:59.302526",
     "exception": false,
     "start_time": "2020-11-29T08:01:15.365283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.6.0)\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (0.23.2)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.18.5)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (4.45.0)\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.4.1)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (0.18.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (0.14.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (2.1.0)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-2.0.0\r\n",
      "Processing /kaggle/input/iterative-stratification/iterative-stratification-master\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.18.5)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.4.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (0.23.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (0.14.1)\r\n",
      "Building wheels for collected packages: iterative-stratification\r\n",
      "  Building wheel for iterative-stratification (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for iterative-stratification: filename=iterative_stratification-0.1.6-py3-none-any.whl size=8401 sha256=17e6006028c5c554df8af9e7108ce082df17fe38fbf83050b943df8b0fbb4909\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/47/3f/eb4af42d124f37d23d6f13a4c8bbc32c1d70140e6e1cecb4aa\r\n",
      "Successfully built iterative-stratification\r\n",
      "Installing collected packages: iterative-stratification\r\n",
      "Successfully installed iterative-stratification-0.1.6\r\n"
     ]
    }
   ],
   "source": [
    "# basics\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import Counter\n",
    "# import copy\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "\n",
    "# sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# pyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CyclicLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# TabNet\n",
    "!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# stratified kfold\n",
    "!pip install /kaggle/input/iterative-stratification/iterative-stratification-master/\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-11-29T08:01:59.363006Z",
     "iopub.status.busy": "2020-11-29T08:01:59.361912Z",
     "iopub.status.idle": "2020-11-29T08:01:59.364718Z",
     "shell.execute_reply": "2020-11-29T08:01:59.365306Z"
    },
    "papermill": {
     "duration": 0.039471,
     "end_time": "2020-11-29T08:01:59.365461",
     "exception": false,
     "start_time": "2020-11-29T08:01:59.325990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### General ###\n",
    "import os\n",
    "import copy\n",
    "import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n",
    "\n",
    "### Data Wrangling ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "### Machine Learning ###\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "### Deep Learning ###\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# Tabnet \n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "from pickle import load,dump\n",
    "\n",
    "### Make prettier the prints ###\n",
    "from colorama import Fore\n",
    "c_ = Fore.CYAN\n",
    "m_ = Fore.MAGENTA\n",
    "r_ = Fore.RED\n",
    "b_ = Fore.BLUE\n",
    "y_ = Fore.YELLOW\n",
    "g_ = Fore.GREEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:01:59.426394Z",
     "iopub.status.busy": "2020-11-29T08:01:59.419772Z",
     "iopub.status.idle": "2020-11-29T08:02:08.002528Z",
     "shell.execute_reply": "2020-11-29T08:02:08.003497Z"
    },
    "papermill": {
     "duration": 8.61472,
     "end_time": "2020-11-29T08:02:08.003724",
     "exception": false,
     "start_time": "2020-11-29T08:01:59.389004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features: (23814, 876)\n",
      "train_targets_scored: (23814, 207)\n",
      "train_targets_nonscored: (23814, 332)\n",
      "train_drug: (23814, 2)\n",
      "test_features: (3982, 876)\n",
      "train_features_extra: (3982, 876)\n",
      "sample_submission: (3982, 207)\n"
     ]
    }
   ],
   "source": [
    "# load raw data\n",
    "data_dir = '../input/lish-moa/'\n",
    "train_features = pd.read_csv(data_dir + 'train_features.csv')\n",
    "train_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\n",
    "train_drug = pd.read_csv(data_dir + 'train_drug.csv')\n",
    "test_features = pd.read_csv(data_dir + 'test_features.csv')\n",
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "\n",
    "train_features_extra = pd.read_csv('../input/moa-fe-extra-data/MoA_FE_Extra_Data.csv')\n",
    "\n",
    "# keep only nonscored targets with >0 positive labels\n",
    "keep_list = (np.where(train_targets_nonscored.iloc[:,1:].values.sum(axis=0)>0)[0] + 1).tolist()\n",
    "train_targets_nonscored = train_targets_nonscored.iloc[:,[0]+keep_list]\n",
    "\n",
    "print('train_features: {}'.format(train_features.shape))\n",
    "print('train_targets_scored: {}'.format(train_targets_scored.shape))\n",
    "print('train_targets_nonscored: {}'.format(train_targets_nonscored.shape))\n",
    "print('train_drug: {}'.format(train_drug.shape))\n",
    "print('test_features: {}'.format(test_features.shape))\n",
    "print('train_features_extra: {}'.format(train_features_extra.shape))\n",
    "print('sample_submission: {}'.format(sample_submission.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:02:08.061661Z",
     "iopub.status.busy": "2020-11-29T08:02:08.060769Z",
     "iopub.status.idle": "2020-11-29T08:02:08.455017Z",
     "shell.execute_reply": "2020-11-29T08:02:08.454207Z"
    },
    "papermill": {
     "duration": 0.424999,
     "end_time": "2020-11-29T08:02:08.455175",
     "exception": false,
     "start_time": "2020-11-29T08:02:08.030176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to set single seed for everything\n",
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set seed to 42\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:02:08.508136Z",
     "iopub.status.busy": "2020-11-29T08:02:08.507209Z",
     "iopub.status.idle": "2020-11-29T08:02:08.511192Z",
     "shell.execute_reply": "2020-11-29T08:02:08.511712Z"
    },
    "papermill": {
     "duration": 0.034034,
     "end_time": "2020-11-29T08:02:08.511885",
     "exception": false,
     "start_time": "2020-11-29T08:02:08.477851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define g/c features\n",
    "g_col = [col for col in train_features.columns if col.startswith('g-')]\n",
    "c_col = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:02:08.634077Z",
     "iopub.status.busy": "2020-11-29T08:02:08.595747Z",
     "iopub.status.idle": "2020-11-29T08:02:08.654289Z",
     "shell.execute_reply": "2020-11-29T08:02:08.653686Z"
    },
    "papermill": {
     "duration": 0.119713,
     "end_time": "2020-11-29T08:02:08.654426",
     "exception": false,
     "start_time": "2020-11-29T08:02:08.534713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering(x_train, x_test, seed):\n",
    "    \n",
    "    # set seed\n",
    "    seed_everything(seed)\n",
    "\n",
    "    # sig_id & cp_type\n",
    "    sig_train = x_train[['sig_id', 'cp_type']]\n",
    "    sig_test = x_test[['sig_id', 'cp_type']]\n",
    "    print('sig_id & cp_type', sig_train.shape, sig_test.shape)\n",
    "    \n",
    "    # OHE for cp_time, cp_dose\n",
    "    cp_dose = {'D1': 0, 'D2': 1}\n",
    "    cp_time = {24:0, 48:1, 72:2}\n",
    "    cp_dose_train = x_train.cp_dose.map(cp_dose)\n",
    "    cp_time_train = x_train.cp_time.map(cp_time)\n",
    "    cp_dose_test = x_test.cp_dose.map(cp_dose)\n",
    "    cp_time_test = x_test.cp_time.map(cp_time)\n",
    "    cp_train = pd.DataFrame({'cp_dose':cp_dose_train, 'cp_time':cp_time_train}).reset_index(drop=True)\n",
    "    cp_test = pd.DataFrame({'cp_dose':cp_dose_test, 'cp_time':cp_time_test}).reset_index(drop=True)\n",
    "    print('OHE for cp_time, cp_dose', cp_train.shape, cp_test.shape)\n",
    "    \n",
    "    # RankGauss scaling\n",
    "    n_quantiles = 100\n",
    "    qt = QuantileTransformer(n_quantiles=n_quantiles, random_state=seed, output_distribution='normal').fit(x_train[g_col + c_col])\n",
    "    rg_train = pd.DataFrame(qt.transform(x_train[g_col + c_col]), columns=['rg_' + col for col in g_col + c_col]).reset_index(drop=True)\n",
    "    rg_test = pd.DataFrame(qt.transform(x_test[g_col + c_col]), columns=['rg_' + col for col in g_col + c_col]).reset_index(drop=True)\n",
    "    print('RankGauss scaling', rg_train.shape, rg_test.shape)\n",
    "    print(rg_train.head())\n",
    "    \n",
    "    # PCA for g_col\n",
    "    g_n_comp = 600\n",
    "    rg_g_col = [col for col in rg_train.columns if col.startswith('rg_g')]\n",
    "    pca = PCA(n_components=g_n_comp, random_state=seed).fit(rg_train[rg_g_col])\n",
    "    pca_g_train = pd.DataFrame(pca.transform(rg_train[rg_g_col]), columns=['pca_g-' + str(i) for i in range(g_n_comp)]).reset_index(drop=True)\n",
    "    pca_g_test = pd.DataFrame(pca.transform(rg_test[rg_g_col]), columns=['pca_g-' + str(i) for i in range(g_n_comp)]).reset_index(drop=True)\n",
    "    print('PCA for g_col', pca_g_train.shape, pca_g_test.shape)\n",
    "    print(pca_g_train.head())\n",
    "\n",
    "    # PCA for c_col\n",
    "    c_n_comp = 50\n",
    "    rg_c_col = [col for col in rg_train.columns if col.startswith('rg_c')]\n",
    "    pca = PCA(n_components=c_n_comp, random_state=seed).fit(rg_train[rg_c_col])\n",
    "    pca_c_train = pd.DataFrame(pca.transform(rg_train[rg_c_col]), columns=['pca_c-' + str(i) for i in range(c_n_comp)]).reset_index(drop=True)\n",
    "    pca_c_test = pd.DataFrame(pca.transform(rg_test[rg_c_col]), columns=['pca_c-' + str(i) for i in range(c_n_comp)]).reset_index(drop=True)\n",
    "    print('PCA for c_col', pca_c_train.shape, pca_c_test.shape)\n",
    "    print(pca_c_train.head())\n",
    "\n",
    "    # combine g & c PCA features\n",
    "    pca_gc_train = pd.concat((pca_g_train, pca_c_train),axis=1).reset_index(drop=True)\n",
    "    pca_gc_test  =pd.concat((pca_g_test, pca_c_test),axis=1).reset_index(drop=True)\n",
    "    print('combine g & c PCA features', pca_gc_train.shape, pca_gc_test.shape)\n",
    "    print(pca_gc_train.head())\n",
    "\n",
    "    # FS by Variance Threshold\n",
    "    thsld = 0.85\n",
    "    data_train = pd.concat([rg_train, pca_gc_train], axis=1)\n",
    "    data_test = pd.concat([rg_test, pca_gc_test], axis=1)\n",
    "    variance_threshold = VarianceThreshold(thsld).fit(data_train)\n",
    "    selected_features = variance_threshold.get_support(True).tolist()\n",
    "    fs_train = data_train.iloc[:,selected_features].reset_index(drop=True)\n",
    "    fs_test = data_test.iloc[:,selected_features].reset_index(drop=True)\n",
    "    print('FS by Variance Threshold', fs_train.shape, fs_test.shape)\n",
    "    print(fs_train.head())\n",
    "\n",
    "    # function to cluster raw g/c\n",
    "    n_clusters_g = 22\n",
    "    n_clusters_c = 4\n",
    "    def gc_cluster(train, test, n_clusters_g, n_clusters_c, seed):\n",
    "\n",
    "        def create_cluster(train, test, features, kind, n_clusters):\n",
    "            col_prefix = 'raw_clusters_'\n",
    "            train_ = train[features].copy()\n",
    "            test_ = test[features].copy()\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=seed).fit(train_)\n",
    "            data = pd.concat([train_, test_], axis=0).reset_index(drop=True)\n",
    "            pred = pd.DataFrame(kmeans.predict(data[features]), columns=[col_prefix + kind])\n",
    "            pred = pd.get_dummies(pred, columns=[col_prefix + kind])\n",
    "            pred_train = pred.iloc[:len(train),:].reset_index(drop=True)\n",
    "            pred_test = pred.iloc[len(train):,:].reset_index(drop=True)\n",
    "            return pred_train, pred_test\n",
    "\n",
    "        g_pred_train, g_pred_test = create_cluster(train, test, g_col, kind='g', n_clusters=n_clusters_g)\n",
    "        c_pred_train, c_pred_test = create_cluster(train, test, c_col, kind='c', n_clusters=n_clusters_c)\n",
    "        gc_pred_train = pd.concat([g_pred_train, c_pred_train], axis=1).reset_index(drop=True)\n",
    "        gc_pred_test = pd.concat([g_pred_test, c_pred_test], axis=1).reset_index(drop=True)\n",
    "        return gc_pred_train, gc_pred_test\n",
    "    \n",
    "    raw_gc_cluster_train, raw_gc_cluster_test = gc_cluster(x_train, x_test, n_clusters_g=n_clusters_g, n_clusters_c=n_clusters_c, seed=seed)\n",
    "    print('cluster raw g/c', raw_gc_cluster_train.shape, raw_gc_cluster_test.shape)\n",
    "\n",
    "\n",
    "    # function to cluster PCA g/c\n",
    "    n_clusters = 5\n",
    "    def fe_cluster_pca(train, test, n_clusters, seed):\n",
    "        col_prefix = 'pca_clusters'\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state = seed).fit(train)\n",
    "        data = pd.concat([train, test], axis=0).reset_index(drop=True)\n",
    "        pred = pd.DataFrame(kmeans.predict(data), columns=[col_prefix])\n",
    "        pred = pd.get_dummies(pred, columns=[col_prefix])\n",
    "        pred_train = pred.iloc[:len(train),:].reset_index(drop=True)\n",
    "        pred_test = pred.iloc[len(train):,:].reset_index(drop=True)\n",
    "        return pred_train, pred_test\n",
    "    pca_cluster_train, pca_cluster_test = fe_cluster_pca(pca_gc_train, pca_gc_test, n_clusters=n_clusters, seed=seed)\n",
    "    print('cluster PCA g/c', pca_cluster_train.shape, pca_cluster_test.shape)\n",
    "\n",
    "\n",
    "    # statistics features\n",
    "    gsquarecols = ['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203',\n",
    "                   'g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22',\n",
    "                   'g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17',\n",
    "                   'g-549','g-145','g-157','g-768','g-568','g-396']\n",
    "    def fe_stats(df):\n",
    "        stat_df = dict()\n",
    "        stat_df['g_sum'] = df[g_col].sum(axis = 1)\n",
    "        stat_df['g_mean'] = df[g_col].mean(axis = 1)\n",
    "        stat_df['g_std'] = df[g_col].std(axis = 1)\n",
    "        stat_df['g_kurt'] = df[g_col].kurtosis(axis = 1)\n",
    "        stat_df['g_skew'] = df[g_col].skew(axis = 1)\n",
    "        stat_df['c_sum'] = df[c_col].sum(axis = 1)\n",
    "        stat_df['c_mean'] = df[c_col].mean(axis = 1)\n",
    "        stat_df['c_std'] = df[c_col].std(axis = 1)\n",
    "        stat_df['c_kurt'] = df[c_col].kurtosis(axis = 1)\n",
    "        stat_df['c_skew'] = df[c_col].skew(axis = 1)\n",
    "        stat_df['gc_sum'] = df[g_col + c_col].sum(axis = 1)\n",
    "        stat_df['gc_mean'] = df[g_col + c_col].mean(axis = 1)\n",
    "        stat_df['gc_std'] = df[g_col + c_col].std(axis = 1)\n",
    "        stat_df['gc_kurt'] = df[g_col + c_col].kurtosis(axis = 1)\n",
    "        stat_df['gc_skew'] = df[g_col + c_col].skew(axis = 1)\n",
    "\n",
    "        stat_df['c52_c42'] = df['c-52'] * df['c-42']\n",
    "        stat_df['c13_c73'] = df['c-13'] * df['c-73']\n",
    "        stat_df['c26_c13'] = df['c-23'] * df['c-13']\n",
    "        stat_df['c33_c6'] = df['c-33'] * df['c-6']\n",
    "        stat_df['c11_c55'] = df['c-11'] * df['c-55']\n",
    "        stat_df['c38_c63'] = df['c-38'] * df['c-63']\n",
    "        stat_df['c38_c94'] = df['c-38'] * df['c-94']\n",
    "        stat_df['c13_c94'] = df['c-13'] * df['c-94']\n",
    "        stat_df['c4_c52'] = df['c-4'] * df['c-52']\n",
    "        stat_df['c4_c42'] = df['c-4'] * df['c-42']\n",
    "        stat_df['c13_c38'] = df['c-13'] * df['c-38']\n",
    "        stat_df['c55_c2'] = df['c-55'] * df['c-2']\n",
    "        stat_df['c55_c4'] = df['c-55'] * df['c-4']\n",
    "        stat_df['c4_c13'] = df['c-4'] * df['c-13']\n",
    "        stat_df['c82_c42'] = df['c-82'] * df['c-42']\n",
    "        stat_df['c66_c42'] = df['c-66'] * df['c-42']\n",
    "        stat_df['c6_c38'] = df['c-6'] * df['c-38']\n",
    "        stat_df['c2_c13'] = df['c-2'] * df['c-13']\n",
    "        stat_df['c62_c42'] = df['c-62'] * df['c-42']\n",
    "        stat_df['c90_c55'] = df['c-90'] * df['c-55']\n",
    "        \n",
    "        for feature in c_col:\n",
    "            stat_df[f'{feature}_squared'] = df[feature] ** 2     \n",
    "        for feature in gsquarecols:\n",
    "            stat_df[f'{feature}_squared'] = df[feature] ** 2  \n",
    "            \n",
    "        stat_df = pd.DataFrame(stat_df)\n",
    "        return stat_df\n",
    "\n",
    "    stat_train, stat_test = fe_stats(x_train), fe_stats(x_test)\n",
    "    print('statistics features', stat_train.shape, stat_test.shape)\n",
    "\n",
    "    # combine all FE results\n",
    "    x_train_fe = pd.concat([sig_train, cp_train, fs_train, raw_gc_cluster_train, pca_cluster_train, stat_train], axis=1).reset_index(drop=True)\n",
    "    x_test_fe = pd.concat([sig_test, cp_test, fs_test, raw_gc_cluster_test, pca_cluster_test, stat_test], axis=1).reset_index(drop=True)\n",
    "    print('combine all FE results', x_train_fe.shape, x_test_fe.shape)\n",
    "\n",
    "    # remove ctrl in train and test\n",
    "    x_train_fe = x_train_fe[x_train_fe.cp_type!='ctl_vehicle']\n",
    "    x_test_fe = x_test_fe[x_test_fe.cp_type!='ctl_vehicle']\n",
    "    x_train_fe = x_train_fe.drop('cp_type', axis=1).reset_index(drop=True)\n",
    "    x_test_fe = x_test_fe.drop('cp_type', axis=1).reset_index(drop=True)\n",
    "    print('remove ctrl in train and test', x_train_fe.shape, x_test_fe.shape)\n",
    "\n",
    "    return x_train_fe, x_test_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:02:08.713570Z",
     "iopub.status.busy": "2020-11-29T08:02:08.708996Z",
     "iopub.status.idle": "2020-11-29T08:05:47.722103Z",
     "shell.execute_reply": "2020-11-29T08:05:47.721346Z"
    },
    "papermill": {
     "duration": 219.04494,
     "end_time": "2020-11-29T08:05:47.722286",
     "exception": false,
     "start_time": "2020-11-29T08:02:08.677346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sig_id & cp_type (27796, 2) (23814, 2)\n",
      "OHE for cp_time, cp_dose (27796, 2) (23814, 2)\n",
      "RankGauss scaling (27796, 872) (23814, 872)\n",
      "     rg_g-0    rg_g-1    rg_g-2    rg_g-3    rg_g-4    rg_g-5    rg_g-6  \\\n",
      "0  1.146806  0.902075 -0.418339 -0.961202 -0.254770 -1.021300 -1.369236   \n",
      "1  0.128824  0.676862  0.274345  0.090495  1.208863  0.688965  0.316734   \n",
      "2  0.790372  0.939951  1.428097 -0.121817 -0.002067  1.495091  0.238763   \n",
      "3 -0.729866 -0.277163 -0.441200  0.766612  2.347817 -0.862761 -2.308829   \n",
      "4 -0.444558 -0.481202  0.974729  0.977467  1.468304 -0.874772 -0.372682   \n",
      "\n",
      "     rg_g-7    rg_g-8    rg_g-9  ...   rg_c-90   rg_c-91   rg_c-92   rg_c-93  \\\n",
      "0 -0.029888  0.684319 -0.316668  ...  0.405455  0.362189  1.296097  0.830281   \n",
      "1  0.556428 -0.539718  0.831972  ... -0.527074  1.127076  0.716060  0.047538   \n",
      "2  0.363471 -0.003611  1.237966  ... -0.834469 -0.747431  0.952950  0.046551   \n",
      "3  0.305225 -0.191898 -1.389591  ... -1.429097 -0.762287 -1.653318 -1.259768   \n",
      "4 -0.212171 -1.067075  0.844018  ...  0.013943  0.000436  1.049064  1.677765   \n",
      "\n",
      "    rg_c-94   rg_c-95   rg_c-96   rg_c-97   rg_c-98   rg_c-99  \n",
      "0 -0.242026  1.015974 -0.506553  0.311033  0.537989  0.644894  \n",
      "1  0.410130  0.741295  0.200866  0.175029  0.914180  1.163177  \n",
      "2 -1.221419 -0.391112 -0.763788 -0.282154 -1.132628  1.086688  \n",
      "3 -0.949084 -1.235487 -1.327725 -0.983066 -0.489388 -0.921288  \n",
      "4  0.792976 -0.374676  0.144687  0.422382 -0.479788  1.118644  \n",
      "\n",
      "[5 rows x 872 columns]\n",
      "PCA for g_col (27796, 600) (23814, 600)\n",
      "     pca_g-0   pca_g-1    pca_g-2   pca_g-3   pca_g-4   pca_g-5   pca_g-6  \\\n",
      "0  -5.509135  3.934672   9.358451 -7.911861  4.834325  0.834668  3.446985   \n",
      "1  -4.899750  3.891428 -11.376574  5.777669  0.924188  0.258293  1.041124   \n",
      "2   1.258620 -7.242684  -5.448532 -0.802672  0.922407  3.698523 -1.905278   \n",
      "3  11.514493 -8.717475  -4.263702 -5.765282 -7.029854 -2.680401 -2.229116   \n",
      "4  -6.541055 -2.337753 -10.742705 -4.184780 -8.086348 -8.202837 -4.266611   \n",
      "\n",
      "    pca_g-7   pca_g-8   pca_g-9  ...  pca_g-590  pca_g-591  pca_g-592  \\\n",
      "0  1.625308  0.909537  2.112349  ...  -0.261101  -0.267146   0.094667   \n",
      "1 -0.395364  5.401568  1.449645  ...   0.133985   0.348230   0.365515   \n",
      "2  2.625305 -4.668135  0.999668  ...  -0.718121   0.514602   0.379937   \n",
      "3  6.561454 -2.617044 -3.505768  ...   0.214487  -0.255354  -0.298730   \n",
      "4 -3.182038 -1.694953  0.846413  ...  -0.450802   0.543815   0.288151   \n",
      "\n",
      "   pca_g-593  pca_g-594  pca_g-595  pca_g-596  pca_g-597  pca_g-598  pca_g-599  \n",
      "0  -0.444712  -0.577729   0.742372  -0.044207   0.617055  -0.827434  -0.597338  \n",
      "1  -0.447731  -0.161037  -0.503646   0.530030  -0.139655  -0.000327  -0.720127  \n",
      "2  -0.296317  -0.516916   0.495590   0.534278  -0.309372  -0.167897  -0.579945  \n",
      "3   0.117346  -0.357698   0.612286  -0.225600   0.540211  -0.695109  -0.110380  \n",
      "4   0.466401   0.956564  -1.072397   0.125651   0.039468   0.031654   0.271205  \n",
      "\n",
      "[5 rows x 600 columns]\n",
      "PCA for c_col (27796, 50) (23814, 50)\n",
      "     pca_c-0   pca_c-1   pca_c-2   pca_c-3   pca_c-4   pca_c-5   pca_c-6  \\\n",
      "0   4.883810  1.541913  1.541407  1.170438  0.913066 -1.064601 -0.229895   \n",
      "1   5.052956 -0.364877 -0.023767  1.002043 -0.532869  0.673775 -0.265609   \n",
      "2  -1.394569  0.297261  0.311774 -0.215324 -0.111797 -0.704118  0.870198   \n",
      "3 -10.982825  1.162351  0.965166 -1.574983  0.006538 -1.255822 -0.380491   \n",
      "4   3.601949  0.563094  0.661240 -0.286989  0.275704  0.276489  0.273743   \n",
      "\n",
      "    pca_c-7   pca_c-8   pca_c-9  ...  pca_c-40  pca_c-41  pca_c-42  pca_c-43  \\\n",
      "0  0.245207 -0.203822 -0.247286  ...  1.022747  0.053285  0.084907 -0.713952   \n",
      "1  0.579524 -1.129011 -0.566306  ... -0.148144  0.757945 -0.499426 -2.088076   \n",
      "2  0.119079 -1.111456 -0.295108  ...  1.386148  0.380597  0.126058 -0.853998   \n",
      "3  0.484322 -0.491942 -1.219179  ... -0.461107  0.003969 -0.584589  0.086056   \n",
      "4 -0.074826 -0.463755  0.189143  ...  1.298892  1.581579 -1.718438  0.290933   \n",
      "\n",
      "   pca_c-44  pca_c-45  pca_c-46  pca_c-47  pca_c-48  pca_c-49  \n",
      "0  0.702275  0.043939  0.073076 -0.993137  0.243027 -0.649340  \n",
      "1 -0.390706 -0.709233 -0.086954  0.187834 -0.310141 -1.098846  \n",
      "2  0.323159 -1.624025 -0.367120 -0.471088  0.110340 -0.682874  \n",
      "3  0.303467 -0.525889 -0.291779  0.359110  0.044018  0.621962  \n",
      "4  0.673369  0.290054 -0.427830  1.561615 -0.137392  0.540456  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "combine g & c PCA features (27796, 650) (23814, 650)\n",
      "     pca_g-0   pca_g-1    pca_g-2   pca_g-3   pca_g-4   pca_g-5   pca_g-6  \\\n",
      "0  -5.509135  3.934672   9.358451 -7.911861  4.834325  0.834668  3.446985   \n",
      "1  -4.899750  3.891428 -11.376574  5.777669  0.924188  0.258293  1.041124   \n",
      "2   1.258620 -7.242684  -5.448532 -0.802672  0.922407  3.698523 -1.905278   \n",
      "3  11.514493 -8.717475  -4.263702 -5.765282 -7.029854 -2.680401 -2.229116   \n",
      "4  -6.541055 -2.337753 -10.742705 -4.184780 -8.086348 -8.202837 -4.266611   \n",
      "\n",
      "    pca_g-7   pca_g-8   pca_g-9  ...  pca_c-40  pca_c-41  pca_c-42  pca_c-43  \\\n",
      "0  1.625308  0.909537  2.112349  ...  1.022747  0.053285  0.084907 -0.713952   \n",
      "1 -0.395364  5.401568  1.449645  ... -0.148144  0.757945 -0.499426 -2.088076   \n",
      "2  2.625305 -4.668135  0.999668  ...  1.386148  0.380597  0.126058 -0.853998   \n",
      "3  6.561454 -2.617044 -3.505768  ... -0.461107  0.003969 -0.584589  0.086056   \n",
      "4 -3.182038 -1.694953  0.846413  ...  1.298892  1.581579 -1.718438  0.290933   \n",
      "\n",
      "   pca_c-44  pca_c-45  pca_c-46  pca_c-47  pca_c-48  pca_c-49  \n",
      "0  0.702275  0.043939  0.073076 -0.993137  0.243027 -0.649340  \n",
      "1 -0.390706 -0.709233 -0.086954  0.187834 -0.310141 -1.098846  \n",
      "2  0.323159 -1.624025 -0.367120 -0.471088  0.110340 -0.682874  \n",
      "3  0.303467 -0.525889 -0.291779  0.359110  0.044018  0.621962  \n",
      "4  0.673369  0.290054 -0.427830  1.561615 -0.137392  0.540456  \n",
      "\n",
      "[5 rows x 650 columns]\n",
      "FS by Variance Threshold (27796, 1024) (23814, 1024)\n",
      "     rg_g-0    rg_g-1    rg_g-2    rg_g-3    rg_g-4    rg_g-5    rg_g-6  \\\n",
      "0  1.146806  0.902075 -0.418339 -0.961202 -0.254770 -1.021300 -1.369236   \n",
      "1  0.128824  0.676862  0.274345  0.090495  1.208863  0.688965  0.316734   \n",
      "2  0.790372  0.939951  1.428097 -0.121817 -0.002067  1.495091  0.238763   \n",
      "3 -0.729866 -0.277163 -0.441200  0.766612  2.347817 -0.862761 -2.308829   \n",
      "4 -0.444558 -0.481202  0.974729  0.977467  1.468304 -0.874772 -0.372682   \n",
      "\n",
      "     rg_g-7    rg_g-8    rg_g-9  ...  pca_g-142  pca_g-143  pca_g-144  \\\n",
      "0 -0.029888  0.684319 -0.316668  ...   0.735437  -0.033264  -0.025932   \n",
      "1  0.556428 -0.539718  0.831972  ...  -1.113827   0.952059   1.285865   \n",
      "2  0.363471 -0.003611  1.237966  ...   0.513153   1.179780   0.965568   \n",
      "3  0.305225 -0.191898 -1.389591  ...  -0.158321  -0.251697   1.203968   \n",
      "4 -0.212171 -1.067075  0.844018  ...  -1.322770  -0.215600  -0.859833   \n",
      "\n",
      "     pca_c-0   pca_c-1   pca_c-2   pca_c-3   pca_c-4   pca_c-5   pca_c-6  \n",
      "0   4.883810  1.541913  1.541407  1.170438  0.913066 -1.064601 -0.229895  \n",
      "1   5.052956 -0.364877 -0.023767  1.002043 -0.532869  0.673775 -0.265609  \n",
      "2  -1.394569  0.297261  0.311774 -0.215324 -0.111797 -0.704118  0.870198  \n",
      "3 -10.982825  1.162351  0.965166 -1.574983  0.006538 -1.255822 -0.380491  \n",
      "4   3.601949  0.563094  0.661240 -0.286989  0.275704  0.276489  0.273743  \n",
      "\n",
      "[5 rows x 1024 columns]\n",
      "cluster raw g/c (27796, 26) (23814, 26)\n",
      "cluster PCA g/c (27796, 5) (23814, 5)\n",
      "statistics features (27796, 180) (23814, 180)\n",
      "combine all FE results (27796, 1239) (23814, 1239)\n",
      "remove ctrl in train and test (25572, 1238) (21948, 1238)\n",
      "sig_id & cp_type (27796, 2) (3982, 2)\n",
      "OHE for cp_time, cp_dose (27796, 2) (3982, 2)\n",
      "RankGauss scaling (27796, 872) (3982, 872)\n",
      "     rg_g-0    rg_g-1    rg_g-2    rg_g-3    rg_g-4    rg_g-5    rg_g-6  \\\n",
      "0  1.146806  0.902075 -0.418339 -0.961202 -0.254770 -1.021300 -1.369236   \n",
      "1  0.128824  0.676862  0.274345  0.090495  1.208863  0.688965  0.316734   \n",
      "2  0.790372  0.939951  1.428097 -0.121817 -0.002067  1.495091  0.238763   \n",
      "3 -0.729866 -0.277163 -0.441200  0.766612  2.347817 -0.862761 -2.308829   \n",
      "4 -0.444558 -0.481202  0.974729  0.977467  1.468304 -0.874772 -0.372682   \n",
      "\n",
      "     rg_g-7    rg_g-8    rg_g-9  ...   rg_c-90   rg_c-91   rg_c-92   rg_c-93  \\\n",
      "0 -0.029888  0.684319 -0.316668  ...  0.405455  0.362189  1.296097  0.830281   \n",
      "1  0.556428 -0.539718  0.831972  ... -0.527074  1.127076  0.716060  0.047538   \n",
      "2  0.363471 -0.003611  1.237966  ... -0.834469 -0.747431  0.952950  0.046551   \n",
      "3  0.305225 -0.191898 -1.389591  ... -1.429097 -0.762287 -1.653318 -1.259768   \n",
      "4 -0.212171 -1.067075  0.844018  ...  0.013943  0.000436  1.049064  1.677765   \n",
      "\n",
      "    rg_c-94   rg_c-95   rg_c-96   rg_c-97   rg_c-98   rg_c-99  \n",
      "0 -0.242026  1.015974 -0.506553  0.311033  0.537989  0.644894  \n",
      "1  0.410130  0.741295  0.200866  0.175029  0.914180  1.163177  \n",
      "2 -1.221419 -0.391112 -0.763788 -0.282154 -1.132628  1.086688  \n",
      "3 -0.949084 -1.235487 -1.327725 -0.983066 -0.489388 -0.921288  \n",
      "4  0.792976 -0.374676  0.144687  0.422382 -0.479788  1.118644  \n",
      "\n",
      "[5 rows x 872 columns]\n",
      "PCA for g_col (27796, 600) (3982, 600)\n",
      "     pca_g-0   pca_g-1    pca_g-2   pca_g-3   pca_g-4   pca_g-5   pca_g-6  \\\n",
      "0  -5.509135  3.934672   9.358451 -7.911861  4.834325  0.834668  3.446985   \n",
      "1  -4.899750  3.891428 -11.376574  5.777669  0.924188  0.258293  1.041124   \n",
      "2   1.258620 -7.242684  -5.448532 -0.802672  0.922407  3.698523 -1.905278   \n",
      "3  11.514493 -8.717475  -4.263702 -5.765282 -7.029854 -2.680401 -2.229116   \n",
      "4  -6.541055 -2.337753 -10.742705 -4.184780 -8.086348 -8.202837 -4.266611   \n",
      "\n",
      "    pca_g-7   pca_g-8   pca_g-9  ...  pca_g-590  pca_g-591  pca_g-592  \\\n",
      "0  1.625308  0.909537  2.112349  ...  -0.261101  -0.267146   0.094667   \n",
      "1 -0.395364  5.401568  1.449645  ...   0.133985   0.348230   0.365515   \n",
      "2  2.625305 -4.668135  0.999668  ...  -0.718121   0.514602   0.379937   \n",
      "3  6.561454 -2.617044 -3.505768  ...   0.214487  -0.255354  -0.298730   \n",
      "4 -3.182038 -1.694953  0.846413  ...  -0.450802   0.543815   0.288151   \n",
      "\n",
      "   pca_g-593  pca_g-594  pca_g-595  pca_g-596  pca_g-597  pca_g-598  pca_g-599  \n",
      "0  -0.444712  -0.577729   0.742372  -0.044207   0.617055  -0.827434  -0.597338  \n",
      "1  -0.447731  -0.161037  -0.503646   0.530030  -0.139655  -0.000327  -0.720127  \n",
      "2  -0.296317  -0.516916   0.495590   0.534278  -0.309372  -0.167897  -0.579945  \n",
      "3   0.117346  -0.357698   0.612286  -0.225600   0.540211  -0.695109  -0.110380  \n",
      "4   0.466401   0.956564  -1.072397   0.125651   0.039468   0.031654   0.271205  \n",
      "\n",
      "[5 rows x 600 columns]\n",
      "PCA for c_col (27796, 50) (3982, 50)\n",
      "     pca_c-0   pca_c-1   pca_c-2   pca_c-3   pca_c-4   pca_c-5   pca_c-6  \\\n",
      "0   4.883810  1.541913  1.541407  1.170438  0.913066 -1.064601 -0.229895   \n",
      "1   5.052956 -0.364877 -0.023767  1.002043 -0.532869  0.673775 -0.265609   \n",
      "2  -1.394569  0.297261  0.311774 -0.215324 -0.111797 -0.704118  0.870198   \n",
      "3 -10.982825  1.162351  0.965166 -1.574983  0.006538 -1.255822 -0.380491   \n",
      "4   3.601949  0.563094  0.661240 -0.286989  0.275704  0.276489  0.273743   \n",
      "\n",
      "    pca_c-7   pca_c-8   pca_c-9  ...  pca_c-40  pca_c-41  pca_c-42  pca_c-43  \\\n",
      "0  0.245207 -0.203822 -0.247286  ...  1.022747  0.053285  0.084907 -0.713952   \n",
      "1  0.579524 -1.129011 -0.566306  ... -0.148144  0.757945 -0.499426 -2.088076   \n",
      "2  0.119079 -1.111456 -0.295108  ...  1.386148  0.380597  0.126058 -0.853998   \n",
      "3  0.484322 -0.491942 -1.219179  ... -0.461107  0.003969 -0.584589  0.086056   \n",
      "4 -0.074826 -0.463755  0.189143  ...  1.298892  1.581579 -1.718438  0.290933   \n",
      "\n",
      "   pca_c-44  pca_c-45  pca_c-46  pca_c-47  pca_c-48  pca_c-49  \n",
      "0  0.702275  0.043939  0.073076 -0.993137  0.243027 -0.649340  \n",
      "1 -0.390706 -0.709233 -0.086954  0.187834 -0.310141 -1.098846  \n",
      "2  0.323159 -1.624025 -0.367120 -0.471088  0.110340 -0.682874  \n",
      "3  0.303467 -0.525889 -0.291779  0.359110  0.044018  0.621962  \n",
      "4  0.673369  0.290054 -0.427830  1.561615 -0.137392  0.540456  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "combine g & c PCA features (27796, 650) (3982, 650)\n",
      "     pca_g-0   pca_g-1    pca_g-2   pca_g-3   pca_g-4   pca_g-5   pca_g-6  \\\n",
      "0  -5.509135  3.934672   9.358451 -7.911861  4.834325  0.834668  3.446985   \n",
      "1  -4.899750  3.891428 -11.376574  5.777669  0.924188  0.258293  1.041124   \n",
      "2   1.258620 -7.242684  -5.448532 -0.802672  0.922407  3.698523 -1.905278   \n",
      "3  11.514493 -8.717475  -4.263702 -5.765282 -7.029854 -2.680401 -2.229116   \n",
      "4  -6.541055 -2.337753 -10.742705 -4.184780 -8.086348 -8.202837 -4.266611   \n",
      "\n",
      "    pca_g-7   pca_g-8   pca_g-9  ...  pca_c-40  pca_c-41  pca_c-42  pca_c-43  \\\n",
      "0  1.625308  0.909537  2.112349  ...  1.022747  0.053285  0.084907 -0.713952   \n",
      "1 -0.395364  5.401568  1.449645  ... -0.148144  0.757945 -0.499426 -2.088076   \n",
      "2  2.625305 -4.668135  0.999668  ...  1.386148  0.380597  0.126058 -0.853998   \n",
      "3  6.561454 -2.617044 -3.505768  ... -0.461107  0.003969 -0.584589  0.086056   \n",
      "4 -3.182038 -1.694953  0.846413  ...  1.298892  1.581579 -1.718438  0.290933   \n",
      "\n",
      "   pca_c-44  pca_c-45  pca_c-46  pca_c-47  pca_c-48  pca_c-49  \n",
      "0  0.702275  0.043939  0.073076 -0.993137  0.243027 -0.649340  \n",
      "1 -0.390706 -0.709233 -0.086954  0.187834 -0.310141 -1.098846  \n",
      "2  0.323159 -1.624025 -0.367120 -0.471088  0.110340 -0.682874  \n",
      "3  0.303467 -0.525889 -0.291779  0.359110  0.044018  0.621962  \n",
      "4  0.673369  0.290054 -0.427830  1.561615 -0.137392  0.540456  \n",
      "\n",
      "[5 rows x 650 columns]\n",
      "FS by Variance Threshold (27796, 1024) (3982, 1024)\n",
      "     rg_g-0    rg_g-1    rg_g-2    rg_g-3    rg_g-4    rg_g-5    rg_g-6  \\\n",
      "0  1.146806  0.902075 -0.418339 -0.961202 -0.254770 -1.021300 -1.369236   \n",
      "1  0.128824  0.676862  0.274345  0.090495  1.208863  0.688965  0.316734   \n",
      "2  0.790372  0.939951  1.428097 -0.121817 -0.002067  1.495091  0.238763   \n",
      "3 -0.729866 -0.277163 -0.441200  0.766612  2.347817 -0.862761 -2.308829   \n",
      "4 -0.444558 -0.481202  0.974729  0.977467  1.468304 -0.874772 -0.372682   \n",
      "\n",
      "     rg_g-7    rg_g-8    rg_g-9  ...  pca_g-142  pca_g-143  pca_g-144  \\\n",
      "0 -0.029888  0.684319 -0.316668  ...   0.735437  -0.033264  -0.025932   \n",
      "1  0.556428 -0.539718  0.831972  ...  -1.113827   0.952059   1.285865   \n",
      "2  0.363471 -0.003611  1.237966  ...   0.513153   1.179780   0.965568   \n",
      "3  0.305225 -0.191898 -1.389591  ...  -0.158321  -0.251697   1.203968   \n",
      "4 -0.212171 -1.067075  0.844018  ...  -1.322770  -0.215600  -0.859833   \n",
      "\n",
      "     pca_c-0   pca_c-1   pca_c-2   pca_c-3   pca_c-4   pca_c-5   pca_c-6  \n",
      "0   4.883810  1.541913  1.541407  1.170438  0.913066 -1.064601 -0.229895  \n",
      "1   5.052956 -0.364877 -0.023767  1.002043 -0.532869  0.673775 -0.265609  \n",
      "2  -1.394569  0.297261  0.311774 -0.215324 -0.111797 -0.704118  0.870198  \n",
      "3 -10.982825  1.162351  0.965166 -1.574983  0.006538 -1.255822 -0.380491  \n",
      "4   3.601949  0.563094  0.661240 -0.286989  0.275704  0.276489  0.273743  \n",
      "\n",
      "[5 rows x 1024 columns]\n",
      "cluster raw g/c (27796, 26) (3982, 26)\n",
      "cluster PCA g/c (27796, 5) (3982, 5)\n",
      "statistics features (27796, 180) (3982, 180)\n",
      "combine all FE results (27796, 1239) (3982, 1239)\n",
      "remove ctrl in train and test (25572, 1238) (3624, 1238)\n"
     ]
    }
   ],
   "source": [
    "# FE parameter estimation based on provided train and test\n",
    "fe_data = train_features.append(train_features_extra).reset_index(drop=True)\n",
    "_, x_train_fe = feature_engineering(fe_data, train_features, seed=42)\n",
    "_, x_test_fe = feature_engineering(fe_data, test_features, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:05:47.793947Z",
     "iopub.status.busy": "2020-11-29T08:05:47.792488Z",
     "iopub.status.idle": "2020-11-29T08:05:48.214334Z",
     "shell.execute_reply": "2020-11-29T08:05:48.215144Z"
    },
    "papermill": {
     "duration": 0.46159,
     "end_time": "2020-11-29T08:05:48.215387",
     "exception": false,
     "start_time": "2020-11-29T08:05:47.753797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# join processed features, scored_targets, non-scored_targets, drug_ids\n",
    "train = x_train_fe.merge(train_targets_scored, on='sig_id')\n",
    "train = train.merge(train_drug, on='sig_id')\n",
    "test = x_test_fe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:05:48.327783Z",
     "iopub.status.busy": "2020-11-29T08:05:48.326683Z",
     "iopub.status.idle": "2020-11-29T08:05:48.331417Z",
     "shell.execute_reply": "2020-11-29T08:05:48.332237Z"
    },
    "papermill": {
     "duration": 0.068458,
     "end_time": "2020-11-29T08:05:48.332464",
     "exception": false,
     "start_time": "2020-11-29T08:05:48.264006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # stratified k-fold by drug_id\n",
    "\n",
    "# SEEDS = [20,21,22]\n",
    "# NFOLDS = 10\n",
    "# DRUG_THRESH = 18\n",
    "\n",
    "# def make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n",
    "#     vc = train.drug_id.value_counts()\n",
    "#     vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n",
    "#     vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n",
    "\n",
    "#     for seed_id in SEEDS:\n",
    "#         kfold_col = 'kfold_{}'.format(seed_id)\n",
    "        \n",
    "#         # STRATIFY DRUGS 18X OR LESS\n",
    "#         dct1 = {}\n",
    "#         dct2 = {}\n",
    "\n",
    "#         skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n",
    "#         tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n",
    "\n",
    "#         for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n",
    "#             dd = {k: fold for k in tmp.index[idxV].values}\n",
    "#             dct1.update(dd)\n",
    "\n",
    "#         # STRATIFY DRUGS MORE THAN 18X\n",
    "#         skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n",
    "#         tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "\n",
    "#         for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n",
    "#             dd = {k: fold for k in tmp.sig_id[idxV].values}\n",
    "#             dct2.update(dd)\n",
    "\n",
    "#         # ASSIGN FOLDS\n",
    "#         train[kfold_col] = train.drug_id.map(dct1)\n",
    "#         train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n",
    "#         train[kfold_col] = train[kfold_col].astype('int8')\n",
    "        \n",
    "#     return train\n",
    "\n",
    "# train = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2020-11-29T08:05:48.440951Z",
     "iopub.status.busy": "2020-11-29T08:05:48.439866Z",
     "iopub.status.idle": "2020-11-29T08:05:48.818878Z",
     "shell.execute_reply": "2020-11-29T08:05:48.817838Z"
    },
    "papermill": {
     "duration": 0.440121,
     "end_time": "2020-11-29T08:05:48.819132",
     "exception": false,
     "start_time": "2020-11-29T08:05:48.379011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>rg_g-0</th>\n",
       "      <th>rg_g-1</th>\n",
       "      <th>rg_g-2</th>\n",
       "      <th>rg_g-3</th>\n",
       "      <th>rg_g-4</th>\n",
       "      <th>rg_g-5</th>\n",
       "      <th>rg_g-6</th>\n",
       "      <th>rg_g-7</th>\n",
       "      <th>...</th>\n",
       "      <th>g-298_squared</th>\n",
       "      <th>g-666_squared</th>\n",
       "      <th>g-91_squared</th>\n",
       "      <th>g-17_squared</th>\n",
       "      <th>g-549_squared</th>\n",
       "      <th>g-145_squared</th>\n",
       "      <th>g-157_squared</th>\n",
       "      <th>g-768_squared</th>\n",
       "      <th>g-568_squared</th>\n",
       "      <th>g-396_squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.146806</td>\n",
       "      <td>0.902075</td>\n",
       "      <td>-0.418339</td>\n",
       "      <td>-0.961202</td>\n",
       "      <td>-0.254770</td>\n",
       "      <td>-1.021300</td>\n",
       "      <td>-1.369236</td>\n",
       "      <td>-0.029888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105820</td>\n",
       "      <td>0.127449</td>\n",
       "      <td>1.261129</td>\n",
       "      <td>0.402210</td>\n",
       "      <td>0.929296</td>\n",
       "      <td>0.030906</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.090481</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.044184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.128824</td>\n",
       "      <td>0.676862</td>\n",
       "      <td>0.274345</td>\n",
       "      <td>0.090495</td>\n",
       "      <td>1.208863</td>\n",
       "      <td>0.688965</td>\n",
       "      <td>0.316734</td>\n",
       "      <td>0.556428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204756</td>\n",
       "      <td>0.025091</td>\n",
       "      <td>0.148533</td>\n",
       "      <td>0.080599</td>\n",
       "      <td>0.081910</td>\n",
       "      <td>0.124186</td>\n",
       "      <td>0.017030</td>\n",
       "      <td>0.026439</td>\n",
       "      <td>0.101761</td>\n",
       "      <td>0.506802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.790372</td>\n",
       "      <td>0.939951</td>\n",
       "      <td>1.428097</td>\n",
       "      <td>-0.121817</td>\n",
       "      <td>-0.002067</td>\n",
       "      <td>1.495091</td>\n",
       "      <td>0.238763</td>\n",
       "      <td>0.363471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047611</td>\n",
       "      <td>0.013572</td>\n",
       "      <td>0.183698</td>\n",
       "      <td>0.094556</td>\n",
       "      <td>0.888495</td>\n",
       "      <td>5.978025</td>\n",
       "      <td>0.129456</td>\n",
       "      <td>0.336052</td>\n",
       "      <td>0.074693</td>\n",
       "      <td>6.801664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.729866</td>\n",
       "      <td>-0.277163</td>\n",
       "      <td>-0.441200</td>\n",
       "      <td>0.766612</td>\n",
       "      <td>2.347817</td>\n",
       "      <td>-0.862761</td>\n",
       "      <td>-2.308829</td>\n",
       "      <td>0.305225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>0.026929</td>\n",
       "      <td>24.167056</td>\n",
       "      <td>0.731709</td>\n",
       "      <td>4.981824</td>\n",
       "      <td>2.982529</td>\n",
       "      <td>0.279524</td>\n",
       "      <td>0.039840</td>\n",
       "      <td>3.243601</td>\n",
       "      <td>0.131044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.444558</td>\n",
       "      <td>-0.481202</td>\n",
       "      <td>0.974729</td>\n",
       "      <td>0.977467</td>\n",
       "      <td>1.468304</td>\n",
       "      <td>-0.874772</td>\n",
       "      <td>-0.372682</td>\n",
       "      <td>-0.212171</td>\n",
       "      <td>...</td>\n",
       "      <td>1.127844</td>\n",
       "      <td>0.527657</td>\n",
       "      <td>0.644006</td>\n",
       "      <td>2.920681</td>\n",
       "      <td>0.365662</td>\n",
       "      <td>0.172391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.004004</td>\n",
       "      <td>0.091023</td>\n",
       "      <td>1.340964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cp_dose  cp_time    rg_g-0    rg_g-1    rg_g-2    rg_g-3    rg_g-4  \\\n",
       "0        0        0  1.146806  0.902075 -0.418339 -0.961202 -0.254770   \n",
       "1        0        2  0.128824  0.676862  0.274345  0.090495  1.208863   \n",
       "2        0        1  0.790372  0.939951  1.428097 -0.121817 -0.002067   \n",
       "3        0        1 -0.729866 -0.277163 -0.441200  0.766612  2.347817   \n",
       "4        1        2 -0.444558 -0.481202  0.974729  0.977467  1.468304   \n",
       "\n",
       "     rg_g-5    rg_g-6    rg_g-7  ...  g-298_squared  g-666_squared  \\\n",
       "0 -1.021300 -1.369236 -0.029888  ...       0.105820       0.127449   \n",
       "1  0.688965  0.316734  0.556428  ...       0.204756       0.025091   \n",
       "2  1.495091  0.238763  0.363471  ...       0.047611       0.013572   \n",
       "3 -0.862761 -2.308829  0.305225  ...       0.230400       0.026929   \n",
       "4 -0.874772 -0.372682 -0.212171  ...       1.127844       0.527657   \n",
       "\n",
       "   g-91_squared  g-17_squared  g-549_squared  g-145_squared  g-157_squared  \\\n",
       "0      1.261129      0.402210       0.929296       0.030906       0.011903   \n",
       "1      0.148533      0.080599       0.081910       0.124186       0.017030   \n",
       "2      0.183698      0.094556       0.888495       5.978025       0.129456   \n",
       "3     24.167056      0.731709       4.981824       2.982529       0.279524   \n",
       "4      0.644006      2.920681       0.365662       0.172391       0.000000   \n",
       "\n",
       "   g-768_squared  g-568_squared  g-396_squared  \n",
       "0       0.090481       0.011300       0.044184  \n",
       "1       0.026439       0.101761       0.506802  \n",
       "2       0.336052       0.074693       6.801664  \n",
       "3       0.039840       3.243601       0.131044  \n",
       "4       1.004004       0.091023       1.340964  \n",
       "\n",
       "[5 rows x 1237 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_cols = [col for col in train_targets_scored.columns if col!='sig_id']\n",
    "feature_cols = [col for col in train.columns if col!='sig_id' and col not in target_cols and col!='drug_id' and 'kfold' not in col]\n",
    "\n",
    "target=train[target_cols]\n",
    "train = train[feature_cols]\n",
    "test = test[feature_cols]\n",
    "X_test = test.values\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:05:48.955826Z",
     "iopub.status.busy": "2020-11-29T08:05:48.954197Z",
     "iopub.status.idle": "2020-11-29T08:05:48.960408Z",
     "shell.execute_reply": "2020-11-29T08:05:48.961783Z"
    },
    "papermill": {
     "duration": 0.088679,
     "end_time": "2020-11-29T08:05:48.962003",
     "exception": false,
     "start_time": "2020-11-29T08:05:48.873324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:05:49.080702Z",
     "iopub.status.busy": "2020-11-29T08:05:49.079367Z",
     "iopub.status.idle": "2020-11-29T08:05:49.085851Z",
     "shell.execute_reply": "2020-11-29T08:05:49.087444Z"
    },
    "papermill": {
     "duration": 0.073282,
     "end_time": "2020-11-29T08:05:49.087676",
     "exception": false,
     "start_time": "2020-11-29T08:05:49.014394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogitsLogLoss(Metric):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._name = \"logits_ll\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        logits = 1 / (1 + np.exp(-y_pred))\n",
    "        aux = (1 - y_true) * np.log(1 - logits + 5e-5) + y_true * np.log(logits + 5e-5)\n",
    "        return np.mean(-aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:05:49.209341Z",
     "iopub.status.busy": "2020-11-29T08:05:49.208277Z",
     "iopub.status.idle": "2020-11-29T08:05:49.213401Z",
     "shell.execute_reply": "2020-11-29T08:05:49.214691Z"
    },
    "papermill": {
     "duration": 0.071248,
     "end_time": "2020-11-29T08:05:49.214896",
     "exception": false,
     "start_time": "2020-11-29T08:05:49.143648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_EPOCH = 200\n",
    "\n",
    "tabnet_params = dict(\n",
    "    n_d = 48,\n",
    "    n_a = 24,\n",
    "    n_steps = 1,\n",
    "    n_independent = 1,\n",
    "    n_shared = 1,\n",
    "    gamma = 1.3,\n",
    "    lambda_sparse = 0,\n",
    "    optimizer_fn = optim.Adam,\n",
    "    optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_params = dict(mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n",
    "    scheduler_fn = ReduceLROnPlateau,\n",
    "    seed = 42,\n",
    "    verbose = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:05:49.318994Z",
     "iopub.status.busy": "2020-11-29T08:05:49.316760Z",
     "iopub.status.idle": "2020-11-29T08:05:49.319767Z",
     "shell.execute_reply": "2020-11-29T08:05:49.320408Z"
    },
    "papermill": {
     "duration": 0.0579,
     "end_time": "2020-11-29T08:05:49.320562",
     "exception": false,
     "start_time": "2020-11-29T08:05:49.262662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # model training\n",
    "\n",
    "# test_cv_preds = []\n",
    "# oof_preds = []\n",
    "# oof_targets = []\n",
    "# scores = []\n",
    "\n",
    "# for seed in SEEDS:\n",
    "#     tabnet_params['seed'] = seed\n",
    "    \n",
    "#     for fold in range(NFOLDS):\n",
    "#         print(\"FOLDS: \",fold,'seed:',seed)\n",
    "\n",
    "#         tr_idx = train[train['kfold_'+str(seed)]!=fold].index\n",
    "#         val_idx = train[train['kfold_'+str(seed)]==fold].index\n",
    "        \n",
    "#         X_train = train.iloc[tr_idx,:][feature_cols].reset_index(drop=True).values\n",
    "#         X_val = train.iloc[val_idx,:][feature_cols].reset_index(drop=True).values\n",
    "#         y_train = train.iloc[tr_idx,:][target_cols].reset_index(drop=True).values\n",
    "#         y_val = train.iloc[val_idx,:][target_cols].reset_index(drop=True).values\n",
    "        \n",
    "#         ### Model ###\n",
    "#         model = TabNetRegressor(**tabnet_params)\n",
    "        \n",
    "#         ### Fit ###\n",
    "#         model.fit(\n",
    "#             X_train = X_train,\n",
    "#             y_train = y_train,\n",
    "#             eval_set = [(X_val, y_val)],\n",
    "#             eval_name = [\"val\"],\n",
    "#             eval_metric = [\"logits_ll\"],\n",
    "#             max_epochs = MAX_EPOCH,\n",
    "#             patience = 30,\n",
    "#             batch_size = 1024, \n",
    "#             virtual_batch_size = 32,\n",
    "#             num_workers = 1,\n",
    "#             drop_last = False,\n",
    "#             loss_fn = SmoothBCEwLogits(smoothing=5e-5))\n",
    "    \n",
    "#         ### Save validation result ###\n",
    "#         preds_val = model.predict(X_val)\n",
    "#         preds_val = 1 / (1 + np.exp(-preds_val))\n",
    "#         oof_preds.append(preds_val)\n",
    "#         oof_targets.append(y_val)\n",
    "#         score = np.min(model.history[\"val_logits_ll\"])\n",
    "#         scores.append(score)\n",
    "        \n",
    "#         ### Save model ###\n",
    "#         saving_path_name = 'TabNet_seed_'+str(seed)+'_fold_'+str(fold)\n",
    "#         saved_filepath = model.save_model(saving_path_name)  \n",
    "# #         loaded_model =  TabNetRegressor()\n",
    "# #         loaded_model.load_model(saved_filepath)\n",
    "\n",
    "#         ### Predict on test ###\n",
    "#         model.load_model(saved_filepath)\n",
    "#         preds_test = model.predict(test[feature_cols].values)\n",
    "#         preds_test = 1 / (1 + np.exp(-preds_test))\n",
    "#         test_cv_preds.append(preds_test)\n",
    "\n",
    "# # oof_preds = np.concatenate(oof_preds)\n",
    "# # oof_targets = np.concatenate(oof_targets)\n",
    "\n",
    "# print('CV score:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:05:49.419846Z",
     "iopub.status.busy": "2020-11-29T08:05:49.414904Z",
     "iopub.status.idle": "2020-11-29T09:39:04.149410Z",
     "shell.execute_reply": "2020-11-29T09:39:04.148031Z"
    },
    "papermill": {
     "duration": 5594.784719,
     "end_time": "2020-11-29T09:39:04.149562",
     "exception": false,
     "start_time": "2020-11-29T08:05:49.364843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m FOLDS:  \u001b[31m 1 \u001b[33m seed: 20\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.27706 | val_logits_ll: 0.02926 |  0:00:02s\n",
      "epoch 10 | loss: 0.01874 | val_logits_ll: 0.01835 |  0:00:20s\n",
      "epoch 20 | loss: 0.01747 | val_logits_ll: 0.01787 |  0:00:37s\n",
      "epoch 30 | loss: 0.01687 | val_logits_ll: 0.01687 |  0:00:57s\n",
      "epoch 40 | loss: 0.01645 | val_logits_ll: 0.01668 |  0:01:15s\n",
      "epoch 50 | loss: 0.01604 | val_logits_ll: 0.01662 |  0:01:32s\n",
      "epoch 60 | loss: 0.01589 | val_logits_ll: 0.01664 |  0:01:51s\n",
      "epoch 70 | loss: 0.01565 | val_logits_ll: 0.01672 |  0:02:09s\n",
      "epoch 80 | loss: 0.01538 | val_logits_ll: 0.01654 |  0:02:27s\n",
      "epoch 90 | loss: 0.01532 | val_logits_ll: 0.01661 |  0:02:45s\n",
      "\n",
      "Early stopping occured at epoch 98 with best_epoch = 78 and best_val_logits_ll = 0.0164\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_20_fold_1.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 2 \u001b[33m seed: 20\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.27999 | val_logits_ll: 0.0293  |  0:00:01s\n",
      "epoch 10 | loss: 0.0189  | val_logits_ll: 0.01981 |  0:00:19s\n",
      "epoch 20 | loss: 0.01741 | val_logits_ll: 0.01937 |  0:00:37s\n",
      "epoch 30 | loss: 0.01669 | val_logits_ll: 0.01695 |  0:00:56s\n",
      "epoch 40 | loss: 0.01649 | val_logits_ll: 0.01667 |  0:01:14s\n",
      "epoch 50 | loss: 0.01625 | val_logits_ll: 0.01666 |  0:01:32s\n",
      "epoch 60 | loss: 0.01594 | val_logits_ll: 0.0164  |  0:01:50s\n",
      "epoch 70 | loss: 0.01571 | val_logits_ll: 0.01645 |  0:02:09s\n",
      "epoch 80 | loss: 0.01534 | val_logits_ll: 0.01643 |  0:02:27s\n",
      "\n",
      "Early stopping occured at epoch 80 with best_epoch = 60 and best_val_logits_ll = 0.0164\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_20_fold_2.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 3 \u001b[33m seed: 20\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.27804 | val_logits_ll: 0.02863 |  0:00:02s\n",
      "epoch 10 | loss: 0.01856 | val_logits_ll: 0.01843 |  0:00:20s\n",
      "epoch 20 | loss: 0.01728 | val_logits_ll: 0.01762 |  0:00:39s\n",
      "epoch 30 | loss: 0.01683 | val_logits_ll: 0.01728 |  0:00:56s\n",
      "epoch 40 | loss: 0.01654 | val_logits_ll: 0.01695 |  0:01:14s\n",
      "epoch 50 | loss: 0.01621 | val_logits_ll: 0.01696 |  0:01:33s\n",
      "epoch 60 | loss: 0.01597 | val_logits_ll: 0.01672 |  0:01:51s\n",
      "epoch 70 | loss: 0.01576 | val_logits_ll: 0.01666 |  0:02:08s\n",
      "epoch 80 | loss: 0.01558 | val_logits_ll: 0.01673 |  0:02:28s\n",
      "epoch 90 | loss: 0.01561 | val_logits_ll: 0.01664 |  0:02:46s\n",
      "epoch 100| loss: 0.01537 | val_logits_ll: 0.01667 |  0:03:03s\n",
      "epoch 110| loss: 0.01503 | val_logits_ll: 0.01653 |  0:03:21s\n",
      "epoch 120| loss: 0.01474 | val_logits_ll: 0.01664 |  0:03:40s\n",
      "\n",
      "Early stopping occured at epoch 121 with best_epoch = 101 and best_val_logits_ll = 0.01649\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_20_fold_3.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 4 \u001b[33m seed: 20\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.27644 | val_logits_ll: 0.0288  |  0:00:01s\n",
      "epoch 10 | loss: 0.01849 | val_logits_ll: 0.01852 |  0:00:19s\n",
      "epoch 20 | loss: 0.01746 | val_logits_ll: 0.01762 |  0:00:39s\n",
      "epoch 30 | loss: 0.01676 | val_logits_ll: 0.01688 |  0:00:57s\n",
      "epoch 40 | loss: 0.01661 | val_logits_ll: 0.01693 |  0:01:15s\n",
      "epoch 50 | loss: 0.01636 | val_logits_ll: 0.01651 |  0:01:32s\n",
      "epoch 60 | loss: 0.01607 | val_logits_ll: 0.01655 |  0:01:52s\n",
      "epoch 70 | loss: 0.01595 | val_logits_ll: 0.01648 |  0:02:09s\n",
      "epoch 80 | loss: 0.01565 | val_logits_ll: 0.01644 |  0:02:27s\n",
      "epoch 90 | loss: 0.01543 | val_logits_ll: 0.01657 |  0:02:45s\n",
      "epoch 100| loss: 0.01521 | val_logits_ll: 0.01649 |  0:03:03s\n",
      "\n",
      "Early stopping occured at epoch 108 with best_epoch = 88 and best_val_logits_ll = 0.01633\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_20_fold_4.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 5 \u001b[33m seed: 20\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.27916 | val_logits_ll: 0.02905 |  0:00:02s\n",
      "epoch 10 | loss: 0.01869 | val_logits_ll: 0.01882 |  0:00:20s\n",
      "epoch 20 | loss: 0.01721 | val_logits_ll: 0.01746 |  0:00:38s\n",
      "epoch 30 | loss: 0.01658 | val_logits_ll: 0.01769 |  0:00:56s\n",
      "epoch 40 | loss: 0.01616 | val_logits_ll: 0.01691 |  0:01:14s\n",
      "epoch 50 | loss: 0.01594 | val_logits_ll: 0.01672 |  0:01:33s\n",
      "epoch 60 | loss: 0.01569 | val_logits_ll: 0.01651 |  0:01:51s\n",
      "epoch 70 | loss: 0.01526 | val_logits_ll: 0.01664 |  0:02:08s\n",
      "epoch 80 | loss: 0.01507 | val_logits_ll: 0.0165  |  0:02:27s\n",
      "\n",
      "Early stopping occured at epoch 81 with best_epoch = 61 and best_val_logits_ll = 0.01643\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_20_fold_5.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 6 \u001b[33m seed: 20\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.2816  | val_logits_ll: 0.03023 |  0:00:01s\n",
      "epoch 10 | loss: 0.01905 | val_logits_ll: 0.01863 |  0:00:19s\n",
      "epoch 20 | loss: 0.01736 | val_logits_ll: 0.0173  |  0:00:37s\n",
      "epoch 30 | loss: 0.01678 | val_logits_ll: 0.01676 |  0:00:56s\n",
      "epoch 40 | loss: 0.0162  | val_logits_ll: 0.01646 |  0:01:14s\n",
      "epoch 50 | loss: 0.01599 | val_logits_ll: 0.01665 |  0:01:31s\n",
      "epoch 60 | loss: 0.01574 | val_logits_ll: 0.01636 |  0:01:51s\n",
      "epoch 70 | loss: 0.01539 | val_logits_ll: 0.01644 |  0:02:08s\n",
      "epoch 80 | loss: 0.01514 | val_logits_ll: 0.01645 |  0:02:26s\n",
      "epoch 90 | loss: 0.01467 | val_logits_ll: 0.01663 |  0:02:44s\n",
      "\n",
      "Early stopping occured at epoch 94 with best_epoch = 74 and best_val_logits_ll = 0.01634\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_20_fold_6.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 7 \u001b[33m seed: 20\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.27762 | val_logits_ll: 0.0284  |  0:00:01s\n",
      "epoch 10 | loss: 0.01852 | val_logits_ll: 0.01984 |  0:00:19s\n",
      "epoch 20 | loss: 0.01727 | val_logits_ll: 0.02055 |  0:00:37s\n",
      "epoch 30 | loss: 0.01661 | val_logits_ll: 0.01677 |  0:00:56s\n",
      "epoch 40 | loss: 0.01631 | val_logits_ll: 0.01656 |  0:01:14s\n",
      "epoch 50 | loss: 0.01625 | val_logits_ll: 0.01688 |  0:01:31s\n",
      "epoch 60 | loss: 0.01615 | val_logits_ll: 0.01636 |  0:01:50s\n",
      "epoch 70 | loss: 0.0159  | val_logits_ll: 0.01632 |  0:02:09s\n",
      "epoch 80 | loss: 0.01568 | val_logits_ll: 0.01624 |  0:02:27s\n",
      "epoch 90 | loss: 0.01559 | val_logits_ll: 0.01649 |  0:02:45s\n",
      "epoch 100| loss: 0.01521 | val_logits_ll: 0.01633 |  0:03:04s\n",
      "\n",
      "Early stopping occured at epoch 104 with best_epoch = 84 and best_val_logits_ll = 0.01616\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_20_fold_7.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 8 \u001b[33m seed: 20\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.27796 | val_logits_ll: 0.02808 |  0:00:01s\n",
      "epoch 10 | loss: 0.01841 | val_logits_ll: 0.01818 |  0:00:19s\n",
      "epoch 20 | loss: 0.01719 | val_logits_ll: 0.01733 |  0:00:38s\n",
      "epoch 30 | loss: 0.01659 | val_logits_ll: 0.01697 |  0:00:56s\n",
      "epoch 40 | loss: 0.01622 | val_logits_ll: 0.01693 |  0:01:14s\n",
      "epoch 50 | loss: 0.01602 | val_logits_ll: 0.01652 |  0:01:31s\n",
      "epoch 60 | loss: 0.01563 | val_logits_ll: 0.01639 |  0:01:51s\n",
      "epoch 70 | loss: 0.01553 | val_logits_ll: 0.01653 |  0:02:08s\n",
      "\n",
      "Early stopping occured at epoch 78 with best_epoch = 58 and best_val_logits_ll = 0.01636\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_20_fold_8.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 9 \u001b[33m seed: 20\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.27751 | val_logits_ll: 0.02849 |  0:00:01s\n",
      "epoch 10 | loss: 0.01865 | val_logits_ll: 0.01832 |  0:00:21s\n",
      "epoch 20 | loss: 0.01729 | val_logits_ll: 0.0175  |  0:00:38s\n",
      "epoch 30 | loss: 0.01679 | val_logits_ll: 0.01695 |  0:00:56s\n",
      "epoch 40 | loss: 0.01635 | val_logits_ll: 0.01671 |  0:01:15s\n",
      "epoch 50 | loss: 0.01613 | val_logits_ll: 0.01661 |  0:01:34s\n",
      "epoch 60 | loss: 0.01601 | val_logits_ll: 0.01661 |  0:01:51s\n",
      "epoch 70 | loss: 0.01567 | val_logits_ll: 0.01651 |  0:02:10s\n",
      "epoch 80 | loss: 0.01539 | val_logits_ll: 0.01661 |  0:02:29s\n",
      "epoch 90 | loss: 0.015   | val_logits_ll: 0.01669 |  0:02:47s\n",
      "epoch 100| loss: 0.01485 | val_logits_ll: 0.01642 |  0:03:05s\n",
      "\n",
      "Early stopping occured at epoch 108 with best_epoch = 88 and best_val_logits_ll = 0.01631\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_20_fold_9.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 10 \u001b[33m seed: 20\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.2764  | val_logits_ll: 0.02784 |  0:00:01s\n",
      "epoch 10 | loss: 0.01855 | val_logits_ll: 0.02203 |  0:00:19s\n",
      "epoch 20 | loss: 0.01719 | val_logits_ll: 0.02132 |  0:00:37s\n",
      "epoch 30 | loss: 0.01654 | val_logits_ll: 0.01685 |  0:00:57s\n",
      "epoch 40 | loss: 0.01629 | val_logits_ll: 0.01772 |  0:01:15s\n",
      "epoch 50 | loss: 0.01608 | val_logits_ll: 0.01658 |  0:01:33s\n",
      "epoch 60 | loss: 0.01592 | val_logits_ll: 0.01668 |  0:01:51s\n",
      "epoch 70 | loss: 0.01576 | val_logits_ll: 0.01649 |  0:02:10s\n",
      "epoch 80 | loss: 0.01539 | val_logits_ll: 0.01649 |  0:02:28s\n",
      "\n",
      "Early stopping occured at epoch 85 with best_epoch = 65 and best_val_logits_ll = 0.01632\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_20_fold_10.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 1 \u001b[33m seed: 21\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.3054  | val_logits_ll: 0.0306  |  0:00:01s\n",
      "epoch 10 | loss: 0.01854 | val_logits_ll: 0.01847 |  0:00:21s\n",
      "epoch 20 | loss: 0.01712 | val_logits_ll: 0.01707 |  0:00:38s\n",
      "epoch 30 | loss: 0.0166  | val_logits_ll: 0.01691 |  0:00:56s\n",
      "epoch 40 | loss: 0.01637 | val_logits_ll: 0.0168  |  0:01:15s\n",
      "epoch 50 | loss: 0.01619 | val_logits_ll: 0.01717 |  0:01:33s\n",
      "epoch 60 | loss: 0.01611 | val_logits_ll: 0.01679 |  0:01:51s\n",
      "epoch 70 | loss: 0.01561 | val_logits_ll: 0.01662 |  0:02:09s\n",
      "epoch 80 | loss: 0.01558 | val_logits_ll: 0.01644 |  0:02:28s\n",
      "epoch 90 | loss: 0.0155  | val_logits_ll: 0.01676 |  0:02:46s\n",
      "epoch 100| loss: 0.01531 | val_logits_ll: 0.01653 |  0:03:04s\n",
      "epoch 110| loss: 0.01504 | val_logits_ll: 0.0165  |  0:03:24s\n",
      "epoch 120| loss: 0.01467 | val_logits_ll: 0.01665 |  0:03:41s\n",
      "\n",
      "Early stopping occured at epoch 124 with best_epoch = 104 and best_val_logits_ll = 0.01629\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_21_fold_1.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 2 \u001b[33m seed: 21\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.30586 | val_logits_ll: 0.0302  |  0:00:02s\n",
      "epoch 10 | loss: 0.0185  | val_logits_ll: 0.0181  |  0:00:22s\n",
      "epoch 20 | loss: 0.01748 | val_logits_ll: 0.01751 |  0:00:42s\n",
      "epoch 30 | loss: 0.01686 | val_logits_ll: 0.01699 |  0:01:00s\n",
      "epoch 40 | loss: 0.01649 | val_logits_ll: 0.01677 |  0:01:21s\n",
      "epoch 50 | loss: 0.01627 | val_logits_ll: 0.01668 |  0:01:41s\n",
      "epoch 60 | loss: 0.01587 | val_logits_ll: 0.01639 |  0:02:00s\n",
      "epoch 70 | loss: 0.01574 | val_logits_ll: 0.01655 |  0:02:21s\n",
      "epoch 80 | loss: 0.01537 | val_logits_ll: 0.01661 |  0:02:40s\n",
      "epoch 90 | loss: 0.01512 | val_logits_ll: 0.01655 |  0:02:59s\n",
      "\n",
      "Early stopping occured at epoch 92 with best_epoch = 72 and best_val_logits_ll = 0.01628\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_21_fold_2.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 3 \u001b[33m seed: 21\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.30058 | val_logits_ll: 0.02946 |  0:00:01s\n",
      "epoch 10 | loss: 0.0185  | val_logits_ll: 0.01836 |  0:00:22s\n",
      "epoch 20 | loss: 0.0172  | val_logits_ll: 0.01754 |  0:00:42s\n",
      "epoch 30 | loss: 0.01659 | val_logits_ll: 0.01779 |  0:01:01s\n",
      "epoch 40 | loss: 0.01626 | val_logits_ll: 0.01691 |  0:01:22s\n",
      "epoch 50 | loss: 0.01613 | val_logits_ll: 0.01684 |  0:01:41s\n",
      "epoch 60 | loss: 0.01588 | val_logits_ll: 0.01657 |  0:02:01s\n",
      "epoch 70 | loss: 0.0156  | val_logits_ll: 0.01684 |  0:02:23s\n",
      "epoch 80 | loss: 0.01528 | val_logits_ll: 0.01652 |  0:02:42s\n",
      "epoch 90 | loss: 0.01513 | val_logits_ll: 0.01664 |  0:03:02s\n",
      "epoch 100| loss: 0.0146  | val_logits_ll: 0.01672 |  0:03:22s\n",
      "\n",
      "Early stopping occured at epoch 101 with best_epoch = 81 and best_val_logits_ll = 0.01646\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_21_fold_3.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 4 \u001b[33m seed: 21\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.30323 | val_logits_ll: 0.03028 |  0:00:01s\n",
      "epoch 10 | loss: 0.0191  | val_logits_ll: 0.01869 |  0:00:21s\n",
      "epoch 20 | loss: 0.01748 | val_logits_ll: 0.01772 |  0:00:40s\n",
      "epoch 30 | loss: 0.01698 | val_logits_ll: 0.01903 |  0:01:02s\n",
      "epoch 40 | loss: 0.01657 | val_logits_ll: 0.01689 |  0:01:21s\n",
      "epoch 50 | loss: 0.01652 | val_logits_ll: 0.01669 |  0:01:42s\n",
      "epoch 60 | loss: 0.01596 | val_logits_ll: 0.01645 |  0:02:02s\n",
      "epoch 70 | loss: 0.0156  | val_logits_ll: 0.01655 |  0:02:22s\n",
      "epoch 80 | loss: 0.01532 | val_logits_ll: 0.01654 |  0:02:42s\n",
      "\n",
      "Early stopping occured at epoch 87 with best_epoch = 67 and best_val_logits_ll = 0.01635\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_21_fold_4.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 5 \u001b[33m seed: 21\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.30754 | val_logits_ll: 0.03056 |  0:00:01s\n",
      "epoch 10 | loss: 0.01862 | val_logits_ll: 0.01836 |  0:00:21s\n",
      "epoch 20 | loss: 0.0173  | val_logits_ll: 0.01917 |  0:00:40s\n",
      "epoch 30 | loss: 0.01669 | val_logits_ll: 0.01692 |  0:01:01s\n",
      "epoch 40 | loss: 0.01625 | val_logits_ll: 0.01684 |  0:01:21s\n",
      "epoch 50 | loss: 0.01611 | val_logits_ll: 0.01662 |  0:01:40s\n",
      "epoch 60 | loss: 0.01614 | val_logits_ll: 0.01662 |  0:02:01s\n",
      "epoch 70 | loss: 0.01568 | val_logits_ll: 0.01652 |  0:02:20s\n",
      "epoch 80 | loss: 0.01539 | val_logits_ll: 0.0165  |  0:02:39s\n",
      "epoch 90 | loss: 0.0152  | val_logits_ll: 0.01656 |  0:03:00s\n",
      "\n",
      "Early stopping occured at epoch 98 with best_epoch = 78 and best_val_logits_ll = 0.01643\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_21_fold_5.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 6 \u001b[33m seed: 21\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.30572 | val_logits_ll: 0.03051 |  0:00:02s\n",
      "epoch 10 | loss: 0.01885 | val_logits_ll: 0.01835 |  0:00:23s\n",
      "epoch 20 | loss: 0.01763 | val_logits_ll: 0.0199  |  0:00:47s\n",
      "epoch 30 | loss: 0.0169  | val_logits_ll: 0.01678 |  0:01:08s\n",
      "epoch 40 | loss: 0.01656 | val_logits_ll: 0.01664 |  0:01:31s\n",
      "epoch 50 | loss: 0.01645 | val_logits_ll: 0.01688 |  0:01:53s\n",
      "epoch 60 | loss: 0.01612 | val_logits_ll: 0.01631 |  0:02:14s\n",
      "epoch 70 | loss: 0.01581 | val_logits_ll: 0.01633 |  0:02:37s\n",
      "\n",
      "Early stopping occured at epoch 78 with best_epoch = 58 and best_val_logits_ll = 0.01624\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_21_fold_6.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 7 \u001b[33m seed: 21\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.30353 | val_logits_ll: 0.02927 |  0:00:01s\n",
      "epoch 10 | loss: 0.0188  | val_logits_ll: 0.01898 |  0:00:19s\n",
      "epoch 20 | loss: 0.01727 | val_logits_ll: 0.01759 |  0:00:38s\n",
      "epoch 30 | loss: 0.01673 | val_logits_ll: 0.01897 |  0:00:56s\n",
      "epoch 40 | loss: 0.01646 | val_logits_ll: 0.01714 |  0:01:14s\n",
      "epoch 50 | loss: 0.01613 | val_logits_ll: 0.01648 |  0:01:34s\n",
      "epoch 60 | loss: 0.01585 | val_logits_ll: 0.01632 |  0:01:51s\n",
      "epoch 70 | loss: 0.01558 | val_logits_ll: 0.01631 |  0:02:10s\n",
      "epoch 80 | loss: 0.01546 | val_logits_ll: 0.01631 |  0:02:28s\n",
      "epoch 90 | loss: 0.01499 | val_logits_ll: 0.01634 |  0:02:47s\n",
      "epoch 100| loss: 0.01472 | val_logits_ll: 0.01642 |  0:03:05s\n",
      "\n",
      "Early stopping occured at epoch 103 with best_epoch = 83 and best_val_logits_ll = 0.01615\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_21_fold_7.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 8 \u001b[33m seed: 21\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.30449 | val_logits_ll: 0.03066 |  0:00:01s\n",
      "epoch 10 | loss: 0.01917 | val_logits_ll: 0.01873 |  0:00:21s\n",
      "epoch 20 | loss: 0.01749 | val_logits_ll: 0.01768 |  0:00:38s\n",
      "epoch 30 | loss: 0.0167  | val_logits_ll: 0.01713 |  0:00:57s\n",
      "epoch 40 | loss: 0.01661 | val_logits_ll: 0.01674 |  0:01:16s\n",
      "epoch 50 | loss: 0.01639 | val_logits_ll: 0.01677 |  0:01:34s\n",
      "epoch 60 | loss: 0.01595 | val_logits_ll: 0.01661 |  0:01:52s\n",
      "\n",
      "Early stopping occured at epoch 65 with best_epoch = 45 and best_val_logits_ll = 0.01648\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_21_fold_8.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 9 \u001b[33m seed: 21\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.30598 | val_logits_ll: 0.03053 |  0:00:01s\n",
      "epoch 10 | loss: 0.01883 | val_logits_ll: 0.01823 |  0:00:20s\n",
      "epoch 20 | loss: 0.01713 | val_logits_ll: 0.01744 |  0:00:39s\n",
      "epoch 30 | loss: 0.01652 | val_logits_ll: 0.01903 |  0:00:57s\n",
      "epoch 40 | loss: 0.01621 | val_logits_ll: 0.0166  |  0:01:16s\n",
      "epoch 50 | loss: 0.01592 | val_logits_ll: 0.01666 |  0:01:35s\n",
      "epoch 60 | loss: 0.01571 | val_logits_ll: 0.01648 |  0:01:53s\n",
      "epoch 70 | loss: 0.01569 | val_logits_ll: 0.01657 |  0:02:12s\n",
      "epoch 80 | loss: 0.01541 | val_logits_ll: 0.01652 |  0:02:31s\n",
      "epoch 90 | loss: 0.01504 | val_logits_ll: 0.01634 |  0:02:49s\n",
      "epoch 100| loss: 0.01484 | val_logits_ll: 0.01645 |  0:03:07s\n",
      "\n",
      "Early stopping occured at epoch 108 with best_epoch = 88 and best_val_logits_ll = 0.01631\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_21_fold_9.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 10 \u001b[33m seed: 21\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.3055  | val_logits_ll: 0.03    |  0:00:01s\n",
      "epoch 10 | loss: 0.01873 | val_logits_ll: 0.019   |  0:00:20s\n",
      "epoch 20 | loss: 0.0173  | val_logits_ll: 0.02049 |  0:00:37s\n",
      "epoch 30 | loss: 0.01667 | val_logits_ll: 0.01744 |  0:00:57s\n",
      "epoch 40 | loss: 0.01647 | val_logits_ll: 0.0169  |  0:01:15s\n",
      "epoch 50 | loss: 0.01625 | val_logits_ll: 0.01661 |  0:01:33s\n",
      "epoch 60 | loss: 0.01604 | val_logits_ll: 0.01662 |  0:01:53s\n",
      "epoch 70 | loss: 0.01582 | val_logits_ll: 0.01648 |  0:02:11s\n",
      "epoch 80 | loss: 0.01592 | val_logits_ll: 0.01663 |  0:02:29s\n",
      "epoch 90 | loss: 0.01549 | val_logits_ll: 0.01633 |  0:02:48s\n",
      "epoch 100| loss: 0.01533 | val_logits_ll: 0.01638 |  0:03:07s\n",
      "epoch 110| loss: 0.01517 | val_logits_ll: 0.0163  |  0:03:24s\n",
      "epoch 120| loss: 0.01498 | val_logits_ll: 0.01656 |  0:03:43s\n",
      "\n",
      "Early stopping occured at epoch 122 with best_epoch = 102 and best_val_logits_ll = 0.01623\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_21_fold_10.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 1 \u001b[33m seed: 22\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.26188 | val_logits_ll: 0.02856 |  0:00:01s\n",
      "epoch 10 | loss: 0.01894 | val_logits_ll: 0.01848 |  0:00:20s\n",
      "epoch 20 | loss: 0.01739 | val_logits_ll: 0.01722 |  0:00:38s\n",
      "epoch 30 | loss: 0.01674 | val_logits_ll: 0.01682 |  0:00:57s\n",
      "epoch 40 | loss: 0.01652 | val_logits_ll: 0.01671 |  0:01:16s\n",
      "epoch 50 | loss: 0.01607 | val_logits_ll: 0.0167  |  0:01:34s\n",
      "epoch 60 | loss: 0.0158  | val_logits_ll: 0.01649 |  0:01:52s\n",
      "epoch 70 | loss: 0.01552 | val_logits_ll: 0.01665 |  0:02:11s\n",
      "epoch 80 | loss: 0.01523 | val_logits_ll: 0.01665 |  0:02:29s\n",
      "\n",
      "Early stopping occured at epoch 81 with best_epoch = 61 and best_val_logits_ll = 0.01641\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_22_fold_1.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 2 \u001b[33m seed: 22\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.26365 | val_logits_ll: 0.02798 |  0:00:01s\n",
      "epoch 10 | loss: 0.01869 | val_logits_ll: 0.01834 |  0:00:21s\n",
      "epoch 20 | loss: 0.01726 | val_logits_ll: 0.01939 |  0:00:39s\n",
      "epoch 30 | loss: 0.01674 | val_logits_ll: 0.01733 |  0:00:57s\n",
      "epoch 40 | loss: 0.01639 | val_logits_ll: 0.01674 |  0:01:15s\n",
      "epoch 50 | loss: 0.01648 | val_logits_ll: 0.0169  |  0:01:34s\n",
      "epoch 60 | loss: 0.01587 | val_logits_ll: 0.01661 |  0:01:53s\n",
      "epoch 70 | loss: 0.01583 | val_logits_ll: 0.01658 |  0:02:10s\n",
      "epoch 80 | loss: 0.01588 | val_logits_ll: 0.01656 |  0:02:30s\n",
      "epoch 90 | loss: 0.01521 | val_logits_ll: 0.01634 |  0:02:48s\n",
      "\n",
      "Early stopping occured at epoch 95 with best_epoch = 75 and best_val_logits_ll = 0.01633\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_22_fold_2.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 3 \u001b[33m seed: 22\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.26189 | val_logits_ll: 0.02787 |  0:00:01s\n",
      "epoch 10 | loss: 0.01885 | val_logits_ll: 0.01862 |  0:00:20s\n",
      "epoch 20 | loss: 0.01731 | val_logits_ll: 0.0203  |  0:00:39s\n",
      "epoch 30 | loss: 0.01673 | val_logits_ll: 0.01742 |  0:00:56s\n",
      "epoch 40 | loss: 0.01625 | val_logits_ll: 0.0168  |  0:01:15s\n",
      "epoch 50 | loss: 0.01605 | val_logits_ll: 0.01665 |  0:01:34s\n",
      "epoch 60 | loss: 0.0157  | val_logits_ll: 0.01698 |  0:01:52s\n",
      "epoch 70 | loss: 0.01545 | val_logits_ll: 0.01676 |  0:02:10s\n",
      "epoch 80 | loss: 0.01515 | val_logits_ll: 0.01675 |  0:02:29s\n",
      "\n",
      "Early stopping occured at epoch 89 with best_epoch = 69 and best_val_logits_ll = 0.01659\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_22_fold_3.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 4 \u001b[33m seed: 22\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.26485 | val_logits_ll: 0.02891 |  0:00:02s\n",
      "epoch 10 | loss: 0.01888 | val_logits_ll: 0.01839 |  0:00:19s\n",
      "epoch 20 | loss: 0.01746 | val_logits_ll: 0.02343 |  0:00:39s\n",
      "epoch 30 | loss: 0.01704 | val_logits_ll: 0.01699 |  0:00:57s\n",
      "epoch 40 | loss: 0.01656 | val_logits_ll: 0.01662 |  0:01:15s\n",
      "epoch 50 | loss: 0.01631 | val_logits_ll: 0.01642 |  0:01:34s\n",
      "epoch 60 | loss: 0.01602 | val_logits_ll: 0.01657 |  0:01:52s\n",
      "epoch 70 | loss: 0.01566 | val_logits_ll: 0.01667 |  0:02:10s\n",
      "epoch 80 | loss: 0.01551 | val_logits_ll: 0.0164  |  0:02:28s\n",
      "epoch 90 | loss: 0.01522 | val_logits_ll: 0.01653 |  0:02:47s\n",
      "\n",
      "Early stopping occured at epoch 91 with best_epoch = 71 and best_val_logits_ll = 0.01634\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_22_fold_4.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 5 \u001b[33m seed: 22\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.26354 | val_logits_ll: 0.02833 |  0:00:01s\n",
      "epoch 10 | loss: 0.01858 | val_logits_ll: 0.01835 |  0:00:20s\n",
      "epoch 20 | loss: 0.01729 | val_logits_ll: 0.01788 |  0:00:37s\n",
      "epoch 30 | loss: 0.01677 | val_logits_ll: 0.01703 |  0:00:56s\n",
      "epoch 40 | loss: 0.01652 | val_logits_ll: 0.01708 |  0:01:14s\n",
      "epoch 50 | loss: 0.01617 | val_logits_ll: 0.01693 |  0:01:32s\n",
      "epoch 60 | loss: 0.01601 | val_logits_ll: 0.01659 |  0:01:52s\n",
      "epoch 70 | loss: 0.01597 | val_logits_ll: 0.01667 |  0:02:10s\n",
      "epoch 80 | loss: 0.01568 | val_logits_ll: 0.01666 |  0:02:28s\n",
      "epoch 90 | loss: 0.01538 | val_logits_ll: 0.01657 |  0:02:47s\n",
      "epoch 100| loss: 0.01535 | val_logits_ll: 0.01656 |  0:03:05s\n",
      "epoch 110| loss: 0.01492 | val_logits_ll: 0.01664 |  0:03:22s\n",
      "epoch 120| loss: 0.01473 | val_logits_ll: 0.01672 |  0:03:41s\n",
      "\n",
      "Early stopping occured at epoch 123 with best_epoch = 103 and best_val_logits_ll = 0.01646\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_22_fold_5.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 6 \u001b[33m seed: 22\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.26282 | val_logits_ll: 0.02819 |  0:00:01s\n",
      "epoch 10 | loss: 0.01889 | val_logits_ll: 0.01909 |  0:00:19s\n",
      "epoch 20 | loss: 0.01744 | val_logits_ll: 0.01848 |  0:00:37s\n",
      "epoch 30 | loss: 0.01671 | val_logits_ll: 0.01666 |  0:00:56s\n",
      "epoch 40 | loss: 0.01645 | val_logits_ll: 0.01645 |  0:01:15s\n",
      "epoch 50 | loss: 0.01615 | val_logits_ll: 0.01647 |  0:01:32s\n",
      "epoch 60 | loss: 0.01594 | val_logits_ll: 0.01642 |  0:01:51s\n",
      "epoch 70 | loss: 0.01562 | val_logits_ll: 0.01626 |  0:02:10s\n",
      "epoch 80 | loss: 0.01556 | val_logits_ll: 0.01636 |  0:02:28s\n",
      "epoch 90 | loss: 0.01514 | val_logits_ll: 0.01622 |  0:02:45s\n",
      "epoch 100| loss: 0.01493 | val_logits_ll: 0.01638 |  0:03:05s\n",
      "epoch 110| loss: 0.01444 | val_logits_ll: 0.01647 |  0:03:22s\n",
      "\n",
      "Early stopping occured at epoch 111 with best_epoch = 91 and best_val_logits_ll = 0.01618\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_22_fold_6.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 7 \u001b[33m seed: 22\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.26378 | val_logits_ll: 0.02839 |  0:00:01s\n",
      "epoch 10 | loss: 0.01866 | val_logits_ll: 0.01893 |  0:00:19s\n",
      "epoch 20 | loss: 0.01715 | val_logits_ll: 0.01857 |  0:00:38s\n",
      "epoch 30 | loss: 0.01663 | val_logits_ll: 0.01659 |  0:00:56s\n",
      "epoch 40 | loss: 0.01625 | val_logits_ll: 0.01649 |  0:01:14s\n",
      "epoch 50 | loss: 0.01602 | val_logits_ll: 0.0165  |  0:01:34s\n",
      "epoch 60 | loss: 0.01575 | val_logits_ll: 0.01624 |  0:01:52s\n",
      "epoch 70 | loss: 0.01563 | val_logits_ll: 0.01628 |  0:02:10s\n",
      "epoch 80 | loss: 0.01552 | val_logits_ll: 0.01623 |  0:02:28s\n",
      "epoch 90 | loss: 0.01519 | val_logits_ll: 0.01627 |  0:02:47s\n",
      "\n",
      "Early stopping occured at epoch 96 with best_epoch = 76 and best_val_logits_ll = 0.01613\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_22_fold_7.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 8 \u001b[33m seed: 22\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.26355 | val_logits_ll: 0.02913 |  0:00:01s\n",
      "epoch 10 | loss: 0.01884 | val_logits_ll: 0.01852 |  0:00:20s\n",
      "epoch 20 | loss: 0.01733 | val_logits_ll: 0.01981 |  0:00:39s\n",
      "epoch 30 | loss: 0.01665 | val_logits_ll: 0.01692 |  0:00:57s\n",
      "epoch 40 | loss: 0.01638 | val_logits_ll: 0.01683 |  0:01:15s\n",
      "epoch 50 | loss: 0.01617 | val_logits_ll: 0.01679 |  0:01:34s\n",
      "epoch 60 | loss: 0.01592 | val_logits_ll: 0.01709 |  0:01:52s\n",
      "epoch 70 | loss: 0.0158  | val_logits_ll: 0.01648 |  0:02:10s\n",
      "epoch 80 | loss: 0.01558 | val_logits_ll: 0.01653 |  0:02:30s\n",
      "epoch 90 | loss: 0.01526 | val_logits_ll: 0.01623 |  0:02:48s\n",
      "epoch 100| loss: 0.01519 | val_logits_ll: 0.01654 |  0:03:06s\n",
      "epoch 110| loss: 0.01467 | val_logits_ll: 0.01662 |  0:03:25s\n",
      "\n",
      "Early stopping occured at epoch 110 with best_epoch = 90 and best_val_logits_ll = 0.01623\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_22_fold_8.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 9 \u001b[33m seed: 22\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.26271 | val_logits_ll: 0.02762 |  0:00:01s\n",
      "epoch 10 | loss: 0.01867 | val_logits_ll: 0.01999 |  0:00:19s\n",
      "epoch 20 | loss: 0.01734 | val_logits_ll: 0.01858 |  0:00:37s\n",
      "epoch 30 | loss: 0.01671 | val_logits_ll: 0.01697 |  0:00:56s\n",
      "epoch 40 | loss: 0.01631 | val_logits_ll: 0.01675 |  0:01:15s\n",
      "epoch 50 | loss: 0.01604 | val_logits_ll: 0.01671 |  0:01:33s\n",
      "epoch 60 | loss: 0.01579 | val_logits_ll: 0.01655 |  0:01:50s\n",
      "epoch 70 | loss: 0.01567 | val_logits_ll: 0.01653 |  0:02:10s\n",
      "epoch 80 | loss: 0.01544 | val_logits_ll: 0.0165  |  0:02:27s\n",
      "epoch 90 | loss: 0.01526 | val_logits_ll: 0.01646 |  0:02:46s\n",
      "epoch 100| loss: 0.01534 | val_logits_ll: 0.01657 |  0:03:05s\n",
      "epoch 110| loss: 0.0148  | val_logits_ll: 0.0164  |  0:03:23s\n",
      "epoch 120| loss: 0.01434 | val_logits_ll: 0.01648 |  0:03:41s\n",
      "\n",
      "Early stopping occured at epoch 121 with best_epoch = 101 and best_val_logits_ll = 0.01628\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_22_fold_9.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\u001b[34m FOLDS:  \u001b[31m 10 \u001b[33m seed: 22\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.2634  | val_logits_ll: 0.02859 |  0:00:02s\n",
      "epoch 10 | loss: 0.01858 | val_logits_ll: 0.01924 |  0:00:20s\n",
      "epoch 20 | loss: 0.01721 | val_logits_ll: 0.01924 |  0:00:39s\n",
      "epoch 30 | loss: 0.01655 | val_logits_ll: 0.01781 |  0:00:57s\n",
      "epoch 40 | loss: 0.01617 | val_logits_ll: 0.01676 |  0:01:16s\n",
      "epoch 50 | loss: 0.01584 | val_logits_ll: 0.01654 |  0:01:34s\n",
      "epoch 60 | loss: 0.01543 | val_logits_ll: 0.01664 |  0:01:52s\n",
      "epoch 70 | loss: 0.01517 | val_logits_ll: 0.0165  |  0:02:11s\n",
      "\n",
      "Early stopping occured at epoch 78 with best_epoch = 58 and best_val_logits_ll = 0.0164\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "Successfully saved model at TabNet_seed_22_fold_10.zip\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n"
     ]
    }
   ],
   "source": [
    "scores_auc_all = []\n",
    "test_cv_preds = []\n",
    "\n",
    "NB_SPLITS = 10\n",
    "mskf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, random_state = 0, shuffle = True)\n",
    "\n",
    "oof_preds = []\n",
    "oof_targets = []\n",
    "scores = []\n",
    "scores_auc = []\n",
    "SEED = [20,21,22]\n",
    "\n",
    "for s in SEED:\n",
    "    tabnet_params['seed'] = s\n",
    "    for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train, target)):\n",
    "        print(b_,\"FOLDS: \", r_, fold_nb + 1, y_, 'seed:', tabnet_params['seed'])\n",
    "        print(g_, '*' * 60, c_)\n",
    "    \n",
    "        X_train, y_train = train.values[train_idx, :], target.values[train_idx, :]\n",
    "        X_val, y_val = train.values[val_idx, :], target.values[val_idx, :]\n",
    "        ### Model ###\n",
    "        model = TabNetRegressor(**tabnet_params)\n",
    "        \n",
    "        ### Fit ###\n",
    "        model.fit(\n",
    "            X_train = X_train,\n",
    "            y_train = y_train,\n",
    "            eval_set = [(X_val, y_val)],\n",
    "            eval_name = [\"val\"],\n",
    "            eval_metric = [\"logits_ll\"],\n",
    "            max_epochs = MAX_EPOCH,\n",
    "            patience = 20,\n",
    "            batch_size = 1024, \n",
    "            virtual_batch_size = 32,\n",
    "            num_workers = 1,\n",
    "            drop_last = False,\n",
    "            loss_fn = SmoothBCEwLogits(smoothing=5e-5))\n",
    "        print(y_, '-' * 60)\n",
    "    \n",
    "        ### Predict on validation ###\n",
    "        preds_val = model.predict(X_val)\n",
    "        # Apply sigmoid to the predictions\n",
    "        preds = 1 / (1 + np.exp(-preds_val))\n",
    "        score = np.min(model.history[\"val_logits_ll\"])\n",
    "        saving_path_name = 'TabNet_seed_'+str(tabnet_params['seed'])+'_fold_'+str(fold_nb+1)\n",
    "        saved_filepath = model.save_model(saving_path_name)\n",
    "        \n",
    "        loaded_model =  TabNetRegressor()\n",
    "        loaded_model.load_model(saved_filepath)\n",
    "    \n",
    "        ### Save OOF for CV ###\n",
    "        oof_preds.append(preds_val)\n",
    "        oof_targets.append(y_val)\n",
    "        scores.append(score)\n",
    "    \n",
    "        ### Predict on test ###\n",
    "        model.load_model(saved_filepath)\n",
    "        preds_test = model.predict(X_test)\n",
    "        test_cv_preds.append(1 / (1 + np.exp(-preds_test)))\n",
    "\n",
    "oof_preds_all = np.concatenate(oof_preds)\n",
    "oof_targets_all = np.concatenate(oof_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T09:39:04.589371Z",
     "iopub.status.busy": "2020-11-29T09:39:04.588115Z",
     "iopub.status.idle": "2020-11-29T09:39:08.938876Z",
     "shell.execute_reply": "2020-11-29T09:39:08.938008Z"
    },
    "papermill": {
     "duration": 4.572531,
     "end_time": "2020-11-29T09:39:08.939011",
     "exception": false,
     "start_time": "2020-11-29T09:39:04.366480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mOverall AUC: \u001b[31m0.7672920606312914\n",
      "\u001b[34mAverage CV: \u001b[31m0.016336569404956192\n"
     ]
    }
   ],
   "source": [
    "aucs = []\n",
    "for task_id in range(oof_preds_all.shape[1]):\n",
    "    aucs.append(roc_auc_score(y_true = oof_targets_all[:, task_id],\n",
    "                              y_score = oof_preds_all[:, task_id]\n",
    "                             ))\n",
    "print(f\"{b_}Overall AUC: {r_}{np.mean(aucs)}\")\n",
    "print(f\"{b_}Average CV: {r_}{np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T09:39:09.382962Z",
     "iopub.status.busy": "2020-11-29T09:39:09.382069Z",
     "iopub.status.idle": "2020-11-29T09:39:09.388499Z",
     "shell.execute_reply": "2020-11-29T09:39:09.387890Z"
    },
    "papermill": {
     "duration": 0.232067,
     "end_time": "2020-11-29T09:39:09.388626",
     "exception": false,
     "start_time": "2020-11-29T09:39:09.156559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65844, 206)\n",
      "(65844, 206)\n",
      "(65844, 206)\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "print(oof_preds_all.shape)\n",
    "print(oof_targets_all.shape)\n",
    "print(oof_preds_all.shape)\n",
    "print(tabnet_params['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T09:39:09.833295Z",
     "iopub.status.busy": "2020-11-29T09:39:09.831650Z",
     "iopub.status.idle": "2020-11-29T09:39:12.768245Z",
     "shell.execute_reply": "2020-11-29T09:39:12.766929Z"
    },
    "papermill": {
     "duration": 3.162695,
     "end_time": "2020-11-29T09:39:12.768386",
     "exception": false,
     "start_time": "2020-11-29T09:39:09.605691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3982, 207)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.016781</td>\n",
       "      <td>0.020182</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>0.006233</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>0.001541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.013677</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.008541</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.006090</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.003420</td>\n",
       "      <td>0.002619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.008685</td>\n",
       "      <td>0.016844</td>\n",
       "      <td>0.003525</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.001711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.015553</td>\n",
       "      <td>0.022895</td>\n",
       "      <td>0.004258</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.001677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0  id_0004d9e33                     0.000853                0.001130   \n",
       "1  id_001897cda                     0.000428                0.000900   \n",
       "2  id_002429b5b                     0.000000                0.000000   \n",
       "3  id_00276f245                     0.000778                0.000885   \n",
       "4  id_0027f1083                     0.001467                0.001504   \n",
       "\n",
       "   acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0        0.002237                        0.016781   \n",
       "1        0.002015                        0.002760   \n",
       "2        0.000000                        0.000000   \n",
       "3        0.001584                        0.008685   \n",
       "4        0.001372                        0.015553   \n",
       "\n",
       "   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                           0.020182                        0.004395   \n",
       "1                           0.001541                        0.001793   \n",
       "2                           0.000000                        0.000000   \n",
       "3                           0.016844                        0.003525   \n",
       "4                           0.022895                        0.004258   \n",
       "\n",
       "   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                    0.002568                       0.006233   \n",
       "1                    0.002338                       0.013677   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.002168                       0.004139   \n",
       "4                    0.003497                       0.001798   \n",
       "\n",
       "   adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
       "0                    0.000267  ...                               0.000827   \n",
       "1                    0.002401  ...                               0.000810   \n",
       "2                    0.000000  ...                               0.000000   \n",
       "3                    0.000274  ...                               0.000640   \n",
       "4                    0.000473  ...                               0.000648   \n",
       "\n",
       "   trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0      0.001380         0.004683           0.000668   \n",
       "1      0.001335         0.004333           0.000901   \n",
       "2      0.000000         0.000000           0.000000   \n",
       "3      0.001580         0.002256           0.017213   \n",
       "4      0.000696         0.002504           0.001366   \n",
       "\n",
       "   tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                   0.000660                               0.000603   \n",
       "1                   0.008541                               0.000565   \n",
       "2                   0.000000                               0.000000   \n",
       "3                   0.004731                               0.000630   \n",
       "4                   0.001450                               0.000665   \n",
       "\n",
       "   vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0         0.000543   0.002035                    0.002883       0.001541  \n",
       "1         0.006090   0.001071                    0.003420       0.002619  \n",
       "2         0.000000   0.000000                    0.000000       0.000000  \n",
       "3         0.001566   0.001853                    0.000449       0.001711  \n",
       "4         0.001367   0.001892                    0.000241       0.001677  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set prediction and submission\n",
    "test_cv_preds = np.stack(test_cv_preds).mean(axis=0)\n",
    "test_cv_preds = pd.DataFrame(test_cv_preds, columns=target_cols)\n",
    "test_cv_preds = pd.concat([x_test_fe[['sig_id']], test_cv_preds], axis=1)\n",
    "sub2 = sample_submission[['sig_id']].merge(test_cv_preds, on = \"sig_id\", how = \"left\")\n",
    "sub2.fillna(0, inplace = True)\n",
    "sub2.to_csv(\"submission.csv\", index = None)\n",
    "print(sub2.shape)\n",
    "sub2.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 5883.695688,
   "end_time": "2020-11-29T09:39:14.208101",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-29T08:01:10.512413",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
